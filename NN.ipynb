{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00000-38193f3d-0feb-453c-ad05-cdba2462106a",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8572,
    "execution_start": 1621732754745,
    "source_hash": "186da1cf",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "from imblearn.over_sampling import SMOTE\nimport numpy as np, pandas as pd\nfrom matplotlib import pyplot\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nimport tempfile\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport sklearn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom numpy import log\nfrom sklearn.metrics import f1_score, make_scorer, confusion_matrix, accuracy_score, precision_score, recall_score, precision_recall_curve\nfrom keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n",
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00001-92c27d39-2a03-4d98-b6e3-3a99f5be74eb",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3088,
    "execution_start": 1621732763322,
    "source_hash": "e0607745",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "df = pd.read_csv(\"C:/Users/szymb/Desktop/Blazej/ML/spotify_preprocessed.csv\")",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Splitting dataset - train, validation, test",
   "metadata": {
    "cell_id": "00002-3f33741e-af45-42a7-a065-6da2791161bb",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00006-b50d92d1-a2f3-4956-960f-7ed81bcb03c8",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1621732767558,
    "source_hash": "6f3e076a",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "def split_dataset(df, sample=1):\n    df = df.sample(int(sample * df.shape[0]))\n    train, validate, test = np.split(df.sample(frac=1, random_state=42), [int(.6*len(df)), int(.8*len(df))])\n\n    print(f\" Train: {train.shape}\\n\", f\"Validate: {validate.shape}\\n\", f\"Test: {test.shape}\\n\")\n\n    Xtrain = train.loc[:, df.columns != 'label']\n\n    Xvalidate = validate.loc[:, df.columns != 'label']\n\n    Xtest = test.loc[:, df.columns != 'label']\n\n\n    ytrain = train.label.values.ravel()\n\n    yvalidate = validate.label.values.ravel()\n\n    ytest = test.label.values.ravel()\n\n    return Xtrain, Xvalidate, Xtest, ytrain, yvalidate, ytest",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00004-379ca93e-7521-4f9a-b428-2493b1299e02",
    "deepnote_cell_type": "code"
   },
   "source": "trainX, validateX, testX, trainy, validatey, testy = split_dataset(df,sample=1)",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": " Train: (345369, 19)\n Validate: (115123, 19)\n Test: (115123, 19)\n\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Although the opposite is unlikely, it is worth making sure that the distribution of the variables labeled as 1 is comparable in all subsets",
   "metadata": {
    "cell_id": "00005-dc8f9fd0-aefb-4117-a4bb-af8d07d96d2c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00006-0fb9fce6-0441-435a-b043-408d15f17896",
    "deepnote_cell_type": "code"
   },
   "source": "round(trainy.sum()/len(trainy),4)",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1006"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00007-de2e3265-1c6a-4651-9db1-5eab7c2f2348",
    "deepnote_cell_type": "code"
   },
   "source": "round(validatey.sum()/len(validatey),4)",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0.1005"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00008-079c0ff0-2387-4b40-b856-5c1392aace5e",
    "deepnote_cell_type": "code"
   },
   "source": "round(testy.sum()/len(testy),4)",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0991"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Data normalization",
   "metadata": {
    "cell_id": "00009-05ab7e4c-796a-47e0-bb6d-d8b2d8e1d0b7",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Although data normalization is not necessary for artificial neural networks, and especially in 'MLP'. \nThis is due to the fact that weights and biases can always be appropriately 'shifted' and scaled up or down. \nHowever, there are practical reasons for this approach - especially speeding up the calculation and reducing the risk of \ngetting stuck at local minima.",
   "metadata": {
    "cell_id": "00010-6ea84bee-7629-4ab8-84de-74f61b792f23",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00011-03df530d-c56e-4711-9624-a00923781275",
    "deepnote_cell_type": "code"
   },
   "source": "s = StandardScaler()\ntrainX = s.fit_transform(trainX)\nvalidateX = s.transform(validateX)\ntestX = s.transform(testX)",
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00012-52528e9c-35a4-4cf9-a5dc-ec3dae25437c",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Model preparation",
   "metadata": {
    "cell_id": "00013-c1daba6c-fa9c-4fef-ad80-6835ed984479",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "Since the ratio of the dependent variables is skewed, additional metrics than accuracy are necessary.\n\nIn order to compare the results of the neural network with other models, we decided that the best metric would be f1. However, it is not built into the library, so custom functions are required.",
   "metadata": {
    "cell_id": "00014-3e8532a9-d562-4d0e-ae0c-d2466f555253",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00015-ac913c90-a358-429f-a889-149d92f71355",
    "deepnote_cell_type": "code"
   },
   "source": "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint \nclass Metrics(Callback):\n    def __init__(self, validation):   \n        super(Metrics, self).__init__()\n        self.validation = validation    \n            \n        print('validation shape', len(self.validation[0]))\n        \n    def on_train_begin(self, logs={}):        \n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n     \n    def on_epoch_end(self, epoch, logs={}):\n        val_targ = self.validation[1]   \n        val_predict = (np.asarray(self.model.predict(self.validation[0]))).round()        \n    \n        val_f1 = f1_score(val_targ, val_predict)\n        val_recall = recall_score(val_targ, val_predict)         \n        val_precision = precision_score(val_targ, val_predict)\n        \n        self.val_f1s.append(round(val_f1, 6))\n        self.val_recalls.append(round(val_recall, 6))\n        self.val_precisions.append(round(val_precision, 6))\n \n        print(f' — val_f1: {val_f1} — val_precision: {val_precision}, — val_recall: {val_recall}')",
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00016-8934f8ce-f382-4533-ade7-cbdddf042918",
    "deepnote_cell_type": "code"
   },
   "source": "from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives / (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00017-2cb87797-e440-4c7d-8adf-f63d189410da",
    "deepnote_cell_type": "code"
   },
   "source": "def f1_loss(y_true, y_pred):\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - K.mean(f1)",
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00018-3a8813d0-f0b0-41f0-a029-ea8465f9f42f",
    "deepnote_cell_type": "code"
   },
   "source": "def f1_weighted_loss(true, pred): #shapes (batch, 4)\n    ground_positives = K.sum(true, axis=0) + K.epsilon()\n    pred_positives = K.sum(pred, axis=0) + K.epsilon()\n    true_positives = K.sum(true * pred, axis=0) + K.epsilon() \n    precision = true_positives / pred_positives \n    recall = true_positives / ground_positives\n    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n    weighted_f1 = f1 * ground_positives / K.sum(ground_positives) \n    weighted_f1 = K.sum(weighted_f1)\n    return 1 - weighted_f1",
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00019-a8d0d750-2677-4259-b46a-65b7052627d2",
    "deepnote_cell_type": "code"
   },
   "source": "def f1_weighted_metric(true, pred):\n    predLabels = K.argmax(pred, axis=-1)\n    pred = K.one_hot(predLabels, 4) \n    ground_positives = K.sum(true, axis=0) + K.epsilon()\n    pred_positives = K.sum(pred, axis=0) + K.epsilon()\n    true_positives = K.sum(true * pred, axis=0) + K.epsilon()\n    precision = true_positives / pred_positives \n    recall = true_positives / ground_positives\n    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n    weighted_f1 = f1 * ground_positives / K.sum(ground_positives) \n    weighted_f1 = K.sum(weighted_f1)\n    return weighted_f1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "Relu function is used for hidden layer as it has the best performance for that architecture (and architecture is best for problem type).\n\nAnd sigmoid function is used for output layer, as the output is binar",
   "metadata": {
    "cell_id": "00020-9167c263-9737-4b66-b608-46de323d4bc8",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00007-59d599f7-1278-4a92-b0fb-96e5036ec53f",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 69,
    "execution_start": 1621732767564,
    "source_hash": "de923b29",
    "tags": [],
    "deepnote_cell_type": "code"
   },
   "source": "# This function is intended to help deciding on the architecture of the network\n\ndef prepare_model (output_bias = None):\n    if output_bias is not None:\n        output_bias = tf.keras.initializers.Constant(output_bias)\n    model = Sequential()\n    model.add(Dense(40, input_dim=18, activation='relu'))\n    model.add(Dense(20, input_dim=40, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', \n                  metrics=[f1,\n                           keras.metrics.Precision(name='precision'),\n                           keras.metrics.Recall(name='recall'),\n                           keras.metrics.AUC(name='auc'),\n                           keras.metrics.AUC(name='prc', curve='PR'),])\n    return model",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00022-53aa8d85-6ac1-47d5-8e4e-299e444c2fea",
    "deepnote_cell_type": "code"
   },
   "source": "model = prepare_model()\nmodel.summary()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00023-b5abd646-bde2-4595-a07d-80a06e1dca00",
    "deepnote_cell_type": "code"
   },
   "source": "model.predict(trainX[:50])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00024-790fa081-17f6-4ab0-b199-57c494cb0421",
    "deepnote_cell_type": "code"
   },
   "source": "results = model.evaluate(trainX, trainy, batch_size=50, verbose=0)\nprint(\"Loss of the library built-in initialization: \" + str(round(results[0],5)))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "As the 'loss' value, although obviously too large to be accepted in the final model, does not exceed .9 (in the iterations so far).\n\nTherefore, it will not be necessary to manually modify the biases before initializing the model - the first few epochs are enough for the model to learn that the ratio of the dependent variables is not comparable (values labeled as 0 and 1)",
   "metadata": {
    "cell_id": "00025-9ae374e2-872e-411e-bbf4-cecec867907a",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00026-999a2264-889d-475e-a12e-671e9b2b5e33",
    "deepnote_cell_type": "code"
   },
   "source": "num_positives = df['label'].sum()\nprint(num_positives)\nnum_negative = len(df['label'])-num_positives\ninitial_bias = np.log([num_positives/num_negative])\ninitial_bias",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00027-efd3b3c2-058f-449d-b585-3340991a3c24",
    "deepnote_cell_type": "code"
   },
   "source": "model = prepare_model(output_bias=initial_bias)\nmodel.predict(trainX[:50])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00028-ade26c73-8fc9-423e-959c-cf0a44a67b5e",
    "deepnote_cell_type": "code"
   },
   "source": "results = model.evaluate(trainX, trainy, batch_size=50, verbose=0)\nprint(\"Loss of the initialization with self output-bias initialization: \" + str(round(results[0],5)))",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "To prevent overfitting, early stopping is used.",
   "metadata": {
    "cell_id": "00029-a16a7a84-52e8-45a6-8e60-9b4bfb4e8f3c",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": "## Grid search - checking best suited number of nods in hidden layer",
   "metadata": {
    "cell_id": "00030-f5f0a9b4-527e-4288-87ef-d57dea135673",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00031-673edc82-393d-4a6f-a6b9-721e9a691656",
    "deepnote_cell_type": "code"
   },
   "source": "from keras import backend as K\nK.set_value(model.optimizer.learning_rate, 0.2)\ndef grid_search(\n    trainX, trainy, validateX, validatey, testX, testy, max_hidden_layers, nods\n):\n    sm = SMOTE(random_state=42, n_jobs=3)\n    trainX, trainy = sm.fit_resample(trainX, trainy)\n    trainX, trainy, validateX, validatey, testX, testy = (\n        trainX.astype(np.float32),\n        trainy.astype(np.float32),\n        validateX.astype(np.float32),\n        validatey.astype(np.float32),\n        testX.astype(np.float32),\n        testy.astype(np.float32),\n    )\n    best_model = [-1, 0]\n    if max_hidden_layers == 1:\n        for nod1 in nods:\n\n            def prepare_model(nod1=nod1):\n                model = Sequential()\n                model.add(Dense(nod1, input_dim=18, activation=\"relu\"))\n                model.add(Dense(1, activation=\"sigmoid\"))\n                model.compile(\n                    loss=f1_loss,\n                    optimizer=\"adam\",\n                    metrics=[\n                        f1,\n                        keras.metrics.Precision(name=\"precision\"),\n                        keras.metrics.Recall(name=\"recall\"),\n                        keras.metrics.AUC(name=\"auc\"),\n                        keras.metrics.AUC(name=\"prc\", curve=\"PR\"),\n                    ],\n                )\n                return model\n\n            model = prepare_model()\n            history = (\n                model.fit(\n                    trainX,\n                    trainy,\n                    callbacks=[Metrics(validation=(validateX, validatey))],\n                    epochs=100,\n                    batch_size=100,\n                    verbose=1,\n                ),\n            )\n            test_results = model.evaluate(testX, testy, verbose=0)\n            if test_results[1] > best_model[0]:\n                best_model = [test_results[1], nod1]\n            print(best_model)\n    if max_hidden_layers == 2:\n        for nod1 in nods:\n            for nod2 in nods:\n\n                def prepare_model(nod1=nod1, nod2=nod2):\n                    model = Sequential()\n                    model.add(Dense(nod1, input_dim=18, activation=\"relu\"))\n                    model.add(Dense(nod2, activation=\"relu\"))\n                    model.add(Dense(1, activation=\"sigmoid\"))\n                    model.compile(\n                        loss=f1_loss,\n                        optimizer=\"adam\",\n                        metrics=[\n                            f1,\n                            keras.metrics.Precision(name=\"precision\"),\n                            keras.metrics.Recall(name=\"recall\"),\n                            keras.metrics.AUC(name=\"auc\"),\n                            keras.metrics.AUC(name=\"prc\", curve=\"PR\"),\n                        ],\n                    )\n                    return model\n\n                model = prepare_model()\n                history = (\n                    model.fit(\n                        trainX,\n                        trainy,\n                        callbacks=[Metrics(validation=(validateX, validatey))],\n                        epochs=100,\n                        batch_size=100,\n                        verbose=1,\n                    ),\n                )\n                tf.keras.callbacks.EarlyStopping(\n                    monitor=\"val_loss\",\n                    verbose=1,\n                    patience=20,\n                    mode=\"max\",\n                    restore_best_weights=True,\n                ),\n                test_results = model.evaluate(testX, testy, verbose=0)\n                if test_results[1] > best_model[0]:\n                    best_model = [test_results[1], nod1, nod2]\n                print(best_model)\n        \n    if max_hidden_layers == 4:\n        for nod1 in nods:\n            for nod2 in nods:\n                for nod3 in nods:\n                    for nod4 in nods:\n\n                        def prepare_model(nod1=nod1, nod2=nod2, nod3=nod3,nod4=nod4):\n                            \n                            model = Sequential()\n                            model.add(Dense(nod1, input_dim=18, activation=\"relu\"))\n                            model.add(Dense(nod2, activation=\"relu\"))\n                            model.add(Dense(nod3, activation=\"relu\"))\n                            model.add(Dense(nod4, activation=\"relu\"))\n                            model.add(Dense(1, activation=\"sigmoid\"))\n                            model.compile(\n                                loss=f1_loss,\n                                optimizer=\"adam\",\n                                metrics=[\n                                    f1,\n                                    keras.metrics.Precision(name=\"precision\"),\n                                    keras.metrics.Recall(name=\"recall\"),\n                                    keras.metrics.AUC(name=\"auc\"),\n                                    keras.metrics.AUC(name=\"prc\", curve=\"PR\"),\n                                ],\n                            )\n                            return model\n\n                        model = prepare_model()\n                        history = (\n                            model.fit(\n                                trainX,\n                                trainy,\n                                callbacks=[Metrics(validation=(validateX, validatey))],\n                                epochs=100,\n                                batch_size=100,\n                                verbose=1,\n                            ),\n                        )\n                        tf.keras.callbacks.EarlyStopping(\n                            monitor=\"val_loss\",\n                            verbose=1,\n                            patience=20,\n                            mode=\"max\",\n                            restore_best_weights=True,\n                        ),\n                        test_results = model.evaluate(testX, testy, verbose=0)\n                        if test_results[1] > best_model[0]:\n                            best_model = [test_results[1], nod1, nod2,nod3,nod4]\n                        print(best_model)\n    return best_model\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00032-8ca323ac-f298-4dcc-81bf-b1cbe93c178e",
    "deepnote_cell_type": "code"
   },
   "source": "nods = [4,5,8,10,12,15,20,30,40]\ngrid_search(trainX,trainy,testX,testy,validateX,validatey, 4 , nods)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00033-f34b1135-7f56-4987-bb23-a2098852c9ff",
    "deepnote_cell_type": "code"
   },
   "source": "nods = [1,2,3,4,5,10,15,20,25,30,35,40,45,50,55,60]\ngrid_search(trainX,trainy,testX,testy,validateX,validatey, 1 , nods)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00034-e883cccb-cd9c-4d12-b353-e02f3eb31fc8",
    "deepnote_cell_type": "code"
   },
   "source": "nods = [2,3,4,5,10,15,20,25,30,35,40,45,50,55,60]\ngrid_search(trainX,trainy,testX,testy,validateX,validatey, 2 , nods)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Running model with best performacne (regarding f1 score)",
   "metadata": {
    "cell_id": "00035-bde4e949-9985-4482-a581-3b5922cba70e",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00036-4d489b63-7e9f-466e-8f2d-d7042b1786c0",
    "deepnote_cell_type": "code"
   },
   "source": "\nsm = SMOTE(random_state=42, n_jobs=3)\ntrainX, trainy = sm.fit_resample(trainX, trainy)\ntrainX, trainy, validateX, validatey, testX, testy = (\n    trainX.astype(np.float32),\n    trainy.astype(np.float32),\n    validateX.astype(np.float32),\n    validatey.astype(np.float32),\n    testX.astype(np.float32),\n    testy.astype(np.float32),)\ndef prepare_model(nod1=40, nod2=30, nod3=20,nod4=10):\n                            model = Sequential()\n                            model.add(Dense(nod1, input_dim=18, activation=\"relu\"))\n                            model.add(Dense(nod2, activation=\"relu\"))\n                            model.add(Dense(nod3, activation=\"relu\"))\n                            model.add(Dense(nod4, activation=\"relu\"))\n                            model.add(Dense(1, activation=\"sigmoid\"))\n                            model.compile(\n                                loss=f1_loss,\n                                optimizer=\"adam\",\n                                metrics=[\n                                    f1,\n                                    keras.metrics.Precision(name=\"precision\"),\n                                    keras.metrics.Recall(name=\"recall\"),\n                                    keras.metrics.AUC(name=\"auc\"),\n                                    keras.metrics.AUC(name=\"prc\", curve=\"PR\"),],)\n                            return model\n\nmodel = prepare_model()\nhistory = (\n    model.fit(\n        trainX,\n        trainy,\n        callbacks=[Metrics(validation=(validateX, validatey))],\n        epochs=150,\n        batch_size=100,\n        verbose=1,),)\ntf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    verbose=1,\n    patience=20,\n    mode=\"max\",\n    restore_best_weights=True,),\ntest_results = model.evaluate(testX, testy, verbose=0)",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "validation shape 115123\nEpoch 1/150\n6213/6213 [==============================] - 11s 1ms/step - loss: 0.2364 - f1: 0.7668 - precision: 0.6409 - recall: 0.8720 - auc: 0.7507 - prc: 0.7081\n — val_f1: 0.3726526611182655 — val_precision: 0.2396242738845631, — val_recall: 0.8377117179398548\nEpoch 2/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1893 - f1: 0.8111 - precision: 0.7336 - recall: 0.8706 - auc: 0.8102 - prc: 0.7520A: 2s - loss: 0.1896 - f1: 0.8109 - precision: 0.7324 - recall: 0.8702 - auc: 0.8099 -  - ETA: 1s - loss: 0.1895 - f1: 0.8109 - precision: 0.7327 - recall: 0.8703 - auc: 0.8100 - prc - ETA: 1s - loss: 0.1895 - f1: 0.8110 - precision: 0.7329 - recall: 0.8\n — val_f1: 0.3690245661996431 — val_precision: 0.23646051287041994, — val_recall: 0.8398721050812306\nEpoch 3/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1862 - f1: 0.8139 - precision: 0.7441 - recall: 0.8749 - auc: 0.8120 - prc: 0.7530\n — val_f1: 0.38164646686500403 — val_precision: 0.24816286255874945, — val_recall: 0.8258727964051158\nEpoch 4/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1839 - f1: 0.8162 - precision: 0.7491 - recall: 0.8769 - auc: 0.8124 - prc: 0.7533A: 9s - loss: 0.1794 - f1: 0.8207 - precision: 0.7480 - recall: 0.8767 - auc: 0.8 - ETA: 7s - loss: 0.1839 - f1: 0.8163 - precision: 0.7481 - recall: 0.8767 - auc: 0. - ETA: 6s - loss: 0.1845 - f1: 0.8157 - precision: 0.7482 - recall: 0.8768 - auc: 0.8123 - prc: 0.753 - ETA: 6s - loss: 0.1845 - f1: 0.8157 - precision: 0.7482 - recall: 0.8768 - auc: 0.8123 - p - ETA: 6s - loss: 0.1844 - f1: 0.8157 - precision: 0 - ETA: 3s - loss: 0.1842 - f1: 0.8159 - precision: 0.7487 - recall: 0.8768 - auc: 0.8124 - prc: 0.7 - ETA: 3s - loss: 0.1842 - f1: 0.8159 - precision: 0.7487 - recall: 0.8768 - auc: 0.8124 - prc: 0.753 - ETA: 3s - loss: 0.1842 - f1: 0.8159 - precision: 0.7487 - recall: 0.8768 - auc: 0.8124 - prc: 0.7 - ETA: 3s - loss: 0.1842 - f1: 0.8160 - precision - ETA: 0s - loss: 0.1839 - f1: 0.8162 - precision: 0.7490 - recall: 0.8769 - auc: 0.8124 - prc: 0 - ETA: 0s - loss: 0.1839 - f1: 0.8162 - precision: 0.7491 - recall: 0.8769 - auc: 0.8124 - prc:\n — val_f1: 0.3635131173970394 — val_precision: 0.23065656095973217, — val_recall: 0.8573280331835464\nEpoch 5/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1836 - f1: 0.8164 - precision: 0.7520 - recall: 0.8785 - auc: 0.8127 - prc: 0.7534A: 8s - loss: 0.1817 - f1: 0.8183 - precision: 0.7517 - recall: 0.8 - ETA: 6s - loss: 0.1838 - f1: 0.8161 - precision: 0.7517 - recall: 0.8782 - ETA: 5s - loss: 0.1841 - f1: 0.8159 - precision: 0.7518 - recall: 0.8782 - auc: 0.812 - ETA: 4s - loss: 0.1840 - f1: 0.8160 - precision: 0.7518 - recall: 0.8783 - auc: - ETA: 3s - loss: 0.1838 - f1: 0.8162 - precision: 0.7519 - recall: 0.8783 - auc: 0.8127 - prc - ETA: 3s - loss: 0.1838 - f1: 0.8162 - precision: 0.7519 - recall: 0.8783 - auc: 0.8127  - ETA: 2s - loss: 0.1837 - f1: 0.8163 - precision: 0.7519 - recall: 0.8784 - auc: 0.8127  - ETA: 1s - loss: 0.1837 - f1: 0.8163 - precision: 0.7520 - recall: 0.8784 - auc:  - ETA: 0s - loss: 0.1836 - f1: 0.8164 - precision: 0.7520 - recall: 0.8785 - auc: 0.8127 \n — val_f1: 0.37574926542605286 — val_precision: 0.24294543796544912, — val_recall: 0.8288109229173868\nEpoch 6/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1821 - f1: 0.8179 - precision: 0.7541 - recall: 0.8794 - auc: 0.8127 - prc: 0.7534A: 7s - loss: 0.1794 - f1: 0.8206 - precision: 0.7537 - recall: 0.8793 - auc: 0.81 - ETA: 7s - loss: 0.1805 - f1: 0.8195 - precision: 0.7537 - recall: 0.8794 - auc: 0.8127 - prc: 0.75 - ETA: 7s - loss: 0.1805 - f1: 0.8195 - preci - ETA: 3s - loss: 0.1819 - f1: 0.8182 - precision: 0.7539 - rec - ETA: 1s - loss: 0.1820 - f1: 0.8180 - precision: 0.7540 - reca\n — val_f1: 0.406048507377068 — val_precision: 0.2764222562404324, — val_recall: 0.7646042170756999\nEpoch 7/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1836 - f1: 0.8164 - precision: 0.7555 - recall: 0.8800 - auc: 0.8125 - prc: 0.7532A: 6s - loss: 0.1864 - f1: 0.8136 - precision: 0.7553 - recall: 0.8799 - auc: 0.8126 - prc - ETA: 5s - loss: 0.1861 - f1: 0.8140 - precision: 0.7553 - recall: 0.8 - ETA: 4s - loss: 0.1851 - f1: 0.8149 - - ETA: 1s - loss: 0.1839 - f1: 0.8161 - precision: 0.7555 - recall: 0.8800 - auc\n — val_f1: 0.3699958338067644 — val_precision: 0.23691613716835622, — val_recall: 0.844192879363982\nEpoch 8/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1811 - f1: 0.8189 - precision: 0.7569 - recall: 0.8806 - auc: 0.8127 - prc: 0.7533A: 9s - loss: 0.1800 - f1: 0.8203 - precision: 0.7567 - recall: 0.8804 - auc: 0.8127  - ETA: 8s - loss: 0.1815 - f1: 0.8185 - precision: 0.7567 - recall: 0.8804 - auc: 0.81 - ETA: 7s - loss: 0.1810 - f1: 0.8190 - precision: 0.7567 - recall: 0.8804 - auc: - ETA: 6s - loss: 0.1806 - f1: 0.8193 - precision: 0.7567 - recall: 0.8804 - auc: 0.8127 - prc: 0.753 - ETA: 6s - loss: 0.1806 - f1: 0.8194 - precision: 0.7567 - recall: 0.8804 - ETA: 4s - loss: 0.1807 - f1 - ETA: 1s - loss: 0.1811 - f1: 0.8189 - precision: 0.7568 - recall: 0.8806 - au\n — val_f1: 0.37614247048209315 — val_precision: 0.2432891865834475, — val_recall: 0.8286380919460767\nEpoch 9/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1811 - f1: 0.8189 - precision: 0.7578 - recall: 0.8810 - auc: 0.8125 - prc: 0.7532A: 0s - loss: 0.1811 - f1: 0.8189 - precision: 0.7578 - recall: 0.8809 - auc: 0.8 - ETA: 0s - loss: 0.1811 - f1: 0.8189 - precision: 0.7578 - recall: 0.8810 - auc: 0.8125 - prc: 0.75\n — val_f1: 0.3705585666967781 — val_precision: 0.23686363198922145, — val_recall: 0.8507604562737643\nEpoch 10/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1809 - f1: 0.8192 - precision: 0.7584 - recall: 0.8816 - auc: 0.8124 - prc: 0.7529A: 8s - loss: 0.1777 - f1: 0.8222 - precision: 0.7583 - recall: 0.8815 - auc: 0.8124 - prc: 0.753 - ETA: 8s - loss: 0.1778 - f1: 0.8220 - precision: 0.7583 - recall: 0.8815 - auc: 0.8 - ETA: 7s - loss: 0.1778 - f1: 0.8222 - precision: 0.7583 - recall: 0.8815 - auc: 0.8124 - prc: 0. - ETA: 7s - loss: 0.1780 - f1: 0.8220 - precision: 0.7583 - recall: 0.8815 -  - ETA: 6s - loss: 0.1795 - f1: 0.8205 - precision: 0.7583 - recall: 0.8815 - auc: 0.8124 - ETA: 5s - loss: 0.1797 - f1: 0.8203 - precision: 0.7583 - recall: 0.8815 - auc: 0.8124 - prc: 0 - ETA: 5s - loss: 0.1798 - f1: 0.8202 - precision: 0.7583 - recall: 0.8815 - auc: 0.8124 -  - ETA: 4s - loss: 0.1799 - f1: 0.8202 - precis - ETA: 1s - loss: 0.1806 - f1: 0.8194 - precision: 0.7584 - recall: 0\n — val_f1: 0.36578778135048223 — val_precision: 0.23228245397055047, — val_recall: 0.8601797442101624\nEpoch 11/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1795 - f1: 0.8205 - precision: 0.7586 - recall: 0.8823 - auc: 0.8120 - prc: 0.7524A: 8s - loss: 0.1750 - f1: 0.8249 - precision: 0.7585 - recall: 0.8821 - auc: 0.8121 -  - ETA: 8s - loss: 0.1779 - f1: 0.8220 - precision: 0\n — val_f1: 0.35557512770829663 — val_precision: 0.22330634098853933, — val_recall: 0.8721914967162115\nEpoch 12/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1814 - f1: 0.8186 - precision: 0.7591 - recall: 0.8828 - auc: 0.8120 - prc: 0.7523A: 9s - loss: 0.1795 - f1: 0.8204 - precision: 0.7590 - recall: 0.8827 - ETA: 7s - loss: 0.1820 - f1: 0.8179 - precision: 0.7590 - recall: 0.8827 - a - ETA: 5s - loss: 0.1819 - f1: 0.8180 - precision: 0.7590 - recall: 0.8828 - auc: 0. - ETA: 5s - loss: 0.1819 - f1: 0.8181 - precision: 0.7590 - recall: 0.8828 - auc: 0.8120 - prc: - ETA: 4s - loss: 0.1819 - f1: 0.8181 - precision: 0 - ETA: 2s - loss: 0.1815 - f1: 0.8185 - precision: 0.7590 - recall: 0.8828 - auc: 0.8120 - prc: 0.752 - ETA: 2s - loss: 0.1815 - f1: 0.8185 - precision: 0.7590 - recall: 0.8828 - auc: 0.8120 - prc: 0 - ETA: 2s - loss: 0.1815 - f1: 0.8185 - precision: 0.7590 - recall: 0.8828 - auc: 0.8 - ETA: 1s - loss: 0.1814 - f1: 0.8186 - precision: 0.7590 - recall: 0.8828 - auc:\n — val_f1: 0.39288210994598033 — val_precision: 0.2602362978138243, — val_recall: 0.8013307984790875\nEpoch 13/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 8s 1ms/step - loss: 0.1808 - f1: 0.8192 - precision: 0.7596 - recall: 0.8828 - auc: 0.8119 - prc: 0.7522A: 8s - loss: 0.1815 - f1: 0.8188 - precis\n — val_f1: 0.3954370964927587 — val_precision: 0.26447076671723657, — val_recall: 0.7833563774628414\nEpoch 14/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1839 - f1: 0.8161 - precision: 0.7602 - recall: 0.8827 - auc: 0.8118 - prc: 0.7522\n — val_f1: 0.3796484515126151 — val_precision: 0.24672330725793917, — val_recall: 0.8231075008641549\nEpoch 15/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1814 - f1: 0.8186 - precision: 0.7607 - recall: 0.8824 - auc: 0.8116 - prc: 0.7521A: 7s - loss: 0.1814 - f1: 0.8187 - precision: 0 - ETA: 4s - loss:\n — val_f1: 0.39809536010932467 — val_precision: 0.26437140183205243, — val_recall: 0.8055651572761839\nEpoch 16/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1821 - f1: 0.8179 - precision: 0.7609 - recall: 0.8826 - auc: 0.8114 - prc: 0.7519A: - ETA: 1s - loss: 0.1822 - f1: 0.8178 - precision: 0.7609 - re\n — val_f1: 0.406028723002819 — val_precision: 0.27318617640909226, — val_recall: 0.7903560318008988\nEpoch 17/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1798 - f1: 0.8202 - precision: 0.7612 - recall: 0.8826 - auc: 0.8113 - prc: 0.7518\n — val_f1: 0.3811167996190023 — val_precision: 0.24735974447478234, — val_recall: 0.8298479087452472\nEpoch 18/150\n6213/6213 [==============================] - 9s 2ms/step - loss: 0.1811 - f1: 0.8189 - precision: 0.7613 - recall: 0.8829 - auc: 0.8111 - prc: 0.7516A: 5s - loss: 0.1799 - f1: 0.8201 - precision: 0.7613 - recall: 0. - ETA: 3s - loss: 0.1805 - f1: 0.8195 - precision: 0.7613 - recall: 0.8829 - auc: 0.8111 - prc: 0 - ETA: 2s - loss: 0.1806 - f1: 0.8194 - precision:  - ETA: 0s - loss: 0.1811 - f1: 0.8189 - precision: 0.7613 - recall: 0.8829 - auc: 0.8111 - prc: 0.751\n — val_f1: 0.3737489934430001 — val_precision: 0.24015670428226482, — val_recall: 0.8422917386795714\nEpoch 19/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1785 - f1: 0.8216 - precision: 0.7613 - recall: 0.8832 - auc: 0.8109 - prc: 0.7513A: 3s - loss: 0.1781 - f1: 0.8220 - pr\n — val_f1: 0.372515486641584 — val_precision: 0.23875870804306523, — val_recall: 0.847044590390598\nEpoch 20/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1823 - f1: 0.8176 - precision: 0.7615 - recall: 0.8833 - auc: 0.8108 - prc: 0.7512: 8s - loss: 0.1786 - f1: 0.8213 - precision: 0.7614 - recall: 0.8833 - auc: 0.810 - ETA: 7s - loss: 0.1795 - f1: 0.8205 - precision: 0.7615 - recall: 0.8833 - auc: 0.8108 - prc: - ETA: 7s - loss: 0.1800 - f1: 0.8199 - precision: 0.7615 - recall: 0.8833 - auc: 0.8108 - prc: 0.75 - ETA: 7s - loss: 0.1801 - f1: 0.8198 - precision: 0.7615 - recall: 0.8833 - auc: 0.8108 - prc: 0. - ETA: 6s - loss: 0.1803 - f1: 0.8197 - precision: 0.7615 - recall: 0.8833 - auc: 0.8108 - prc: 0 - ETA: 6s -  - ETA: 2s - loss: 0.1819 - f1: 0.8180 - precision: 0.7615 - recall: 0.8 - ETA: 1s - loss: 0.1822 - f1: 0.8177 - precision: 0.7615 - recall: 0.8833 - au\n — val_f1: 0.3849418262299742 — val_precision: 0.251585848130159, — val_recall: 0.8191323885240235\nEpoch 21/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1813 - f1: 0.8187 - precision: 0.7615 - recall: 0.8834 - auc: 0.8105 - prc: 0.7509A: 6s - loss: 0.1822 - f1: 0.8179 - precision: 0.7614 - recall: 0.8834 - auc: 0.8106 - prc: 0.7 - ETA: 5s - loss: 0.1821 - f1: 0.8179 - precision: 0.7614 - recall: 0.8834 - auc: 0.810 - ETA: 5s - loss: 0.1820 - f1: 0.8180 - precision: 0.7614 - recall: 0.8834 - auc: 0.8106 - prc: 0.7 - ETA: 5s - loss: 0.1820 - f1: 0.8180 - precision: 0.7614 - recall: 0.8834 - auc: 0.8106 - - ETA: 4s - loss: 0.1819 - f1: 0.8181 - precision: 0.7614 - recall: 0.8834 - auc: 0.8106 - p - ETA: 3s - loss: 0.1818 - f1: 0.8182 - precision: 0.7614 - recall: 0.8834 - auc: 0.8106 - prc: 0 - ETA: 3s - loss: 0.1817 - f1: 0.8183 - precision: 0.7614 - recall: 0.8834 - auc: 0.8106 - pr - ETA: 3s - loss: 0.1816 - f1: 0.8184 - precision: 0.7614 - recall: 0.8834 - au - ETA: 2s - loss: 0.1815 - f1: 0.8185 - precision: 0.7614 - recall: 0.8834 - auc: 0.8 - ETA: 1s - loss: 0.1814 - f1: 0.8186 - precision: 0.7615 - recall: 0.8834 - auc: 0.8106 - prc: 0.7 - ETA: 1s - loss: 0.1814 - f1: 0.8186 - precision: 0.7615 - recall: 0.8834 - auc: 0 - ETA: 0s - loss: 0.1813 - f1: 0.8187 - precision: 0.7615 - recall: 0.8834 - auc: 0.8105 - prc: 0.75 - ETA: 0s - loss: 0.1813 - f1: 0.8187 - precision: 0.7615 - recall: 0.8834 - auc: 0.8105 - prc: 0.750\n — val_f1: 0.39782494299245746 — val_precision: 0.2665413092020214, — val_recall: 0.7839612858624265\nEpoch 22/150\n6213/6213 [==============================] - 9s 2ms/step - loss: 0.1818 - f1: 0.8182 - precision: 0.7617 - recall: 0.8833 - auc: 0.8105 - prc: 0.7509A: 9s - loss: 0. - ETA: 4s\n — val_f1: 0.41067565413431106 — val_precision: 0.2788101790297146, — val_recall: 0.7792084341514\nEpoch 23/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1816 - f1: 0.8184 - precision: 0.7620 - recall: 0.8830 - auc: 0.8103 - prc: 0.7508\n — val_f1: 0.38820418954647146 — val_precision: 0.2538432895366775, — val_recall: 0.8247493950916004\nEpoch 24/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1816 - f1: 0.8184 - precision: 0.7621 - recall: 0.8830 - auc: 0.8102 - prc: 0.7507A: 3s - loss: 0.1812 - f1: 0.8188 - precision: 0.76 - ETA: 1s - loss: 0.1814 - f1: 0.8186 - precision: 0.7621 - recall: 0.\n — val_f1: 0.4094685902118334 — val_precision: 0.2782293088472515, — val_recall: 0.7750604908399585\nEpoch 25/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1835 - f1: 0.8165 - precision: 0.7622 - recall: 0.8829 - auc: 0.8101 - prc: 0.7506\n — val_f1: 0.3850603876991779 — val_precision: 0.25163823521608786, — val_recall: 0.8196508814379537\nEpoch 26/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1856 - f1: 0.8144 - precision: 0.7621 - recall: 0.8829 - auc: 0.8097 - prc: 0.7503\n — val_f1: 0.3575140599416246 — val_precision: 0.2251210328133405, — val_recall: 0.8679571379191151\nEpoch 27/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1785 - f1: 0.8215 - precision: 0.7621 - recall: 0.8830 - auc: 0.8096 - prc: 0.7501A: 7s - loss: 0.1722 - f1: 0.8278 - precision: 0.7621 - recall: 0.8829 - auc: 0.8096 - prc - ETA: 6s - loss: 0.1740 - f1: - ETA: 3s - loss: 0.1774 - f1: 0.8226 - preci - ETA: 0s - loss: 0.1782 - f1: 0.8218 - precision: 0.7621 - recall: 0.8830 - auc: 0.8\n — val_f1: 0.34476610203704017 — val_precision: 0.21404849927940348, — val_recall: 0.8855858969927412\nEpoch 28/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1827 - f1: 0.8173 - precision: 0.7620 - recall: 0.8833 - auc: 0.8094 - prc: 0.7498A: 5s - loss: 0.1838 - f1: 0.8163 - precision: 0.7619 - recall: 0.8833 - auc: 0.8094 - ETA: 4s - loss: 0.1835 - f1: 0.8165 - precision: 0.7619 - recall: 0.8833 - auc: 0.8094 - ETA: 3s - loss: 0.1833 \n — val_f1: 0.3760975326110131 — val_precision: 0.24242272806776444, — val_recall: 0.8384030418250951\nEpoch 29/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1810 - f1: 0.8190 - precision: 0.7621 - recall: 0.8832 - auc: 0.8093 - prc: 0.7498A: 7s - loss: 0.1815 - f1: 0.8186 - precision: 0.76\n — val_f1: 0.3923263982707376 — val_precision: 0.2582852138693522, — val_recall: 0.8155893536121673\nEpoch 30/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1843 - f1: 0.8157 - precision: 0.7621 - recall: 0.8831 - auc: 0.8092 - prc: 0.7497A: 8s - loss: 0.1827 - f1: 0.8174 - precision: 0.7620 - recall: 0.8834 - auc: 0.80 - ETA: 7s - loss: 0.1838 - f1: 0.8162 - precision: 0.7620 - recall: 0.8833 - auc: 0.8092 - ETA: 6s - loss - ETA: 2s - loss: 0.1843 - f1: 0.8157 - precision: 0.7621 - recall: 0.8832 - auc: 0.8092 - prc:  - ETA: 2s - loss: 0.1843 - f1: 0.8157 - precision: 0.7621 - reca - ETA: 0s - loss: 0.1843 - f1: 0.8157 - precision: 0.7621 - recall: 0.8831 - auc: 0.8092 - prc: 0.749\n — val_f1: 0.3783374140142241 — val_precision: 0.24404612684883428, — val_recall: 0.841254752851711\nEpoch 31/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 8s 1ms/step - loss: 0.1792 - f1: 0.8208 - precision: 0.7623 - recall: 0.8829 - auc: 0.8091 - prc: 0.7496A: 7s - loss: 0.1772 - f1: 0.8229 - precision: 0.7622 - recall: 0.883\n — val_f1: 0.3940630953128598 — val_precision: 0.26002154874713374, — val_recall: 0.8133425509851365\nEpoch 32/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1797 - f1: 0.8203 - precision: 0.7626 - recall: 0.8828 - auc: 0.8091 - prc: 0.7497\n — val_f1: 0.3821937435325957 — val_precision: 0.24826783867631852, — val_recall: 0.8298479087452472\nEpoch 33/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1802 - f1: 0.8198 - precision: 0.7625 - recall: 0.8830 - auc: 0.8091 - prc: 0.7496A: 7s - loss: 0.1774 - f1: 0.8225 - precision: 0. - ETA: 4s - loss: 0.1796 - f1: 0 - ETA: 0s - loss: 0.1802 - f1: 0.8198 - precision: 0.7625 - recall: 0.8830 - auc: 0.\n — val_f1: 0.38903757183460924 — val_precision: 0.25597204400884543, — val_recall: 0.8102315935015555\nEpoch 34/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1813 - f1: 0.8187 - precision: 0.7627 - recall: 0.8828 - auc: 0.8091 - prc: 0.7496A: 0s - loss: 0.1813 - f1: 0.8187 - precision: 0.7627 - recall: 0.8828 - auc: 0.8091 - prc: 0.7\n — val_f1: 0.3980743192417631 — val_precision: 0.2649254798752753, — val_recall: 0.8002938126512271\nEpoch 35/150\n6213/6213 [==============================] - 11s 2ms/step - loss: 0.1858 - f1: 0.8142 - precision: 0.7628 - recall: 0.8827 - auc: 0.8089 - prc: 0.7495\n — val_f1: 0.3602954755309326 — val_precision: 0.22910892949410494, — val_recall: 0.8429830625648116\nEpoch 36/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1832 - f1: 0.8168 - precision: 0.7628 - recall: 0.8826 - auc: 0.8088 - prc: 0.7495A: 8s - loss: 0.1858 - f1: 0.8141 - precision: 0.7629 - recall: 0.8825 - auc: 0.8088 -  - ETA: 7s - loss: 0.1858 - f1: 0.8141 - precision: 0.7629 - recall: 0.8825 - auc: 0. - ETA: 6s - loss: 0.1850 - f1: 0.8150 - precision: 0.7629  - ETA: 4s - loss: 0.1837 - f1: 0.8163 - precision: 0.7628 - recall: 0.8826 - auc: 0.8088 - prc: - ETA: 3s - loss: 0.1836 - f1: 0.8164 - precision: 0.7628 - recall: 0.8826 - auc: 0.8088 - ETA: 3s - loss: 0.1834 - f1: 0.81\n — val_f1: 0.3810017133521935 — val_precision: 0.2475790999948216, — val_recall: 0.8263048738333909\nEpoch 37/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1842 - f1: 0.8158 - precision: 0.7629 - recall: 0.8826 - auc: 0.8087 - prc: 0.7493A: 3s - loss: 0.1844 - f1: 0.8156 - precision: 0.7628 - recall: 0.8826 - - ETA: 2s - loss: 0.1843 - f1: 0.8157 - precision: 0.76\n — val_f1: 0.40143788302042055 — val_precision: 0.2678359869390586, — val_recall: 0.8009851365364673\nEpoch 38/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1844 - f1: 0.8156 - precision: 0.7631 - recall: 0.8822 - auc: 0.8086 - prc: 0.7494TA: 9s - loss: 0.1799 - f1: 0.8199 - precision: 0.7631 - recall: 0.8822 - auc: 0.80 - ETA: 8s - loss: 0.1816 - f1: 0.8183 - precision: 0.7631 - recall: 0.8822 - auc: 0.8087 - prc: 0.749 - ETA: 8s - loss: 0.1818 - f1: 0.8182 - precision: 0.7631 - recall: 0.8 - ETA: 6s - loss: 0.1832 - f1: 0.8169 - precision: 0.7631 - recall: 0.8822 - auc: 0.8087  - ETA: 5s - loss - ETA: 1s - loss: 0.1841 - f1: 0.8159 - precision: 0.7631 - recall: 0.8822 \n — val_f1: 0.381484373435842 — val_precision: 0.24826058631921824, — val_recall: 0.8232803318354649\nEpoch 39/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1891 - f1: 0.8109 - precision: 0.7630 - recall: 0.8820 - auc: 0.8084 - prc: 0.7492A: 7s - loss: 0.1893 - f1: 0.8107 - precision: 0.7631 - recall: 0.8820 - auc: 0. - ETA: 6s  - ETA: 1s - loss: 0.1894 - f1: 0.8106 - precision: 0.7630 - recall: 0.8820 - \n — val_f1: 0.3683816090231199 — val_precision: 0.2351801349217741, — val_recall: 0.8495506394745939\nEpoch 40/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1828 - f1: 0.8172 - precision: 0.7627 - recall: 0.8822 - auc: 0.8082 - prc: 0.7489A: 8s - loss: 0.1808 - f1: 0.8192 - precision: 0.7628 - recall: 0.8821 - auc: 0.8082 - prc: 0.74 - ETA: 7s - loss: 0.1810 - f1: 0.8190 - precision: 0.7628 - recall: 0.8821 - auc: - ETA: 6s - loss: 0.1823 - f1: 0.8177 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082 - prc: 0.748 - ETA: 6s - loss: 0.1823 - f1: 0.8177 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082 - prc: 0.7 - ETA: 6s - loss: 0.1823 - f1: 0.8177 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082 - p - ETA: 6s - loss: 0.1824 - f1: 0.8176 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082 - prc: 0.74 - ETA: 6s - loss: 0.1824 - f1: 0.8176 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082 - prc: 0.748 - ETA: 6s - loss: 0.1824 - f1: 0.8176 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082 - prc: 0.748 - ETA: 5s - loss: 0.1824 - f1: 0.8176 - precision: 0.7628 - recall: 0.8822 - au - ETA: 4s - loss: 0.1824 - f1: 0.8176 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082 - prc:  - ETA: 4s - loss: 0.1825 - f1: 0.8176 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082  - ETA: 3s - loss: 0.1825 - f1: 0.8175 - precision: 0.7628 - recall: 0. - ETA: 2s - loss: 0.1827 - f1: 0.8173 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082 - prc: 0.7 - ETA: 1s - loss: 0.1827 - f1: 0.8173 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082 - prc: 0. - ETA: 1s - loss: 0.1827 - f1: 0.8173 - precision: 0.7628 - recall: 0.8822 - auc: 0.8082  - ETA: 1s - loss: 0.1828 - f1: 0.8172 - precision: 0.7628 - recall: 0.8822 - auc:\n — val_f1: 0.40132377275234415 — val_precision: 0.2694575297010636, — val_recall: 0.7859488420324923\nEpoch 41/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1832 - f1: 0.8168 - precision: 0.7627 - recall: 0.8823 - auc: 0.8081 - prc: 0.7488A: 9s - loss: 0.1945 - f1: 0.8057 - precision: 0.7627 - recall: 0.8823 - auc - ETA: 7s - loss: 0.1837 - f1: 0.8164 - precision: 0.7627 - recall: 0.88 - ETA: 5s - loss: 0.1824 - f1: 0.8177 - precision: 0.7627 - recall: 0.8823 - auc: 0.8081 -  - ETA: 5s - loss: 0.1823 - f1: 0.8178 - precision: 0.7627 - recall: 0.8823 - auc: 0.8081 - prc: 0.7 - ETA: 5s - loss: 0.1823 - f1: 0.8178 - precision: 0.7627 - recall: 0.8823 - auc:  - ETA: 4s - loss: 0.1824 - f1: 0.8176 - precision: 0.7627 - recall: 0.8823 - auc: 0.8081 - prc: 0. - ETA: 3s - loss: 0.1825 - f1: 0.8175 - precision: 0.7627 - recall: 0.8823 - au - ETA: 2s - loss: 0.1828 - f1: 0.8172 - \n — val_f1: 0.3887132995092739 — val_precision: 0.2559419496166484, — val_recall: 0.8077255444175596\nEpoch 42/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1853 - f1: 0.8146 - precision: 0.7626 - recall: 0.8823 - auc: 0.8080 - prc: 0.7486A: 7s - loss: 0.1819 - f1: 0.8181 - precision: 0.7627 - recall:  - ETA: 5s - loss: 0.1842 - f1: 0.8158 - precision: 0.7627 - recall: 0.8823 - auc: 0. - ETA: 4s - loss: 0.1847 - f1: 0.8153 - precision: 0.7627 - recall: 0.8823 - auc: 0.8080 - - ETA: 3s - loss: 0.1850 - f1: 0.8150 - precision: 0.7627 - recall: 0.8823 - auc:  - ETA: 3s - loss: 0.1852 - f1: 0.8148 - precision: 0.7627 - recall: 0.8823 - auc: 0.8080 - prc: 0.748 - ETA: 3s - loss: 0.1852 - f1: 0.8148 - precision: 0.7627 - recall: 0.8823 - auc: 0.8080 - prc: 0.7 - ETA: 2s - loss: 0.1852 - f1: 0.8148 - precision: 0.7627 - recall: 0.8823 - auc: 0.8080 - prc: - ETA: 2s - loss: 0.1853 - f1: 0.8147 - precision: 0.7 - ETA: 0s - loss: 0.1853 - f1: 0.8146 - precision: 0.7626 - recall: 0.8823 - auc: 0.8080 - prc: 0.\n — val_f1: 0.378543878484234 — val_precision: 0.24582967515364354, — val_recall: 0.8226754234358797\nEpoch 43/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1822 - f1: 0.8178 - precision: 0.7626 - recall: 0.8823 - auc: 0.8079 - prc: 0.7485A: 0s - loss: 0.1822 - f1: 0.8177 - precision: 0.7626 - recall: 0.8823 - auc: 0.8079 - prc: 0\n — val_f1: 0.3718754177249031 — val_precision: 0.23868121093271233, — val_recall: 0.8414275838230211\nEpoch 44/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 9s 1ms/step - loss: 0.1848 - f1: 0.8152 - precision: 0.7626 - recall: 0.8823 - auc: 0.8078 - prc: 0.7484A: 3s - loss: 0.1850 - f1: 0.8150 - precision: 0.7626 - recall: 0.8823 - auc: 0.8078 - p - ETA: 2s - loss: 0.1850 - f1: 0.8150 - precision: 0\n — val_f1: 0.37766573149437915 — val_precision: 0.24429554049595167, — val_recall: 0.8317490494296578\nEpoch 45/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1842 - f1: 0.8158 - precision: 0.7626 - recall: 0.8824 - auc: 0.8077 - prc: 0.7483A: 5s - loss: 0.1843 - f1: 0.8158 - precision: 0.7626 - recall: 0.8824 - auc: 0.8077 - prc: 0 - ETA: 5s - loss: 0.1843 - f1: 0.8157 - precision: 0.7626 - recall: 0.8824 - auc: 0 - ETA: 4s - loss: 0.1844 - f1: 0.8156 - precision: 0.7626 - recall: 0.8824 - auc: 0.8077 - prc: - ETA: 4s - loss: 0.1844 - f1: 0.8156 - precision: 0.7626 - recall: 0.8824 - auc: 0.8077 -  - ETA: 3s - loss: 0.1844 - f1: 0.8156 - precision: 0.7626 - recall: 0.8824 - auc: 0.8077 - prc:  - ETA: 3s - loss: 0.1844 - f1: 0.8156 - precision: 0.7626 - recall: 0.8824 - auc: 0 - ETA: 2s - loss: 0.1844 - f1: 0.8156 - precision: 0.7626 - recall: 0.8824 - auc: 0.8077 - ETA: 1s - loss: 0.1843 - f1: 0.8157 - precision: 0.7626 - recall: 0.8824 - ETA: 0s - loss: 0.1842 - f1: 0.8158 - precision: 0.7626 - recall: 0.8824 - auc: 0.8077 - prc: 0.7\n — val_f1: 0.3771815475611417 — val_precision: 0.24338983050847457, — val_recall: 0.8376253024541997\nEpoch 46/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1841 - f1: 0.8159 - precision: 0.7627 - recall: 0.8822 - auc: 0.8077 - prc: 0.7483A: 2s - loss: 0.1839 - f1: 0.8161 - precision: 0.7627 - recall: 0.8822 - auc: 0.80 - ETA: 1s - loss: 0.1840 - f1: 0.8160 - precision: 0.7627 - recall: 0.8822 - auc: 0.8077 - ETA: 0s - loss: 0.1840 - f1: 0.8160 - precision: 0.7627 - recall: 0.8822 - auc: 0.8\n — val_f1: 0.36508437447788067 — val_precision: 0.2324861094692044, — val_recall: 0.8497234704459039\nEpoch 47/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1827 - f1: 0.8173 - precision: 0.7628 - recall: 0.8821 - auc: 0.8076 - prc: 0.7483A: 7s - loss: 0.1807 - f1: 0.8193 - precision: 0.7627\n — val_f1: 0.3998853312163712 — val_precision: 0.26844504973946, — val_recall: 0.7835292084341514\nEpoch 48/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1817 - f1: 0.8183 - precision: 0.7627 - recall: 0.8821 - auc: 0.8075 - prc: 0.7482\n — val_f1: 0.38069852220610717 — val_precision: 0.2461850443599493, — val_recall: 0.8392671966816454\nEpoch 49/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1812 - f1: 0.8188 - precision: 0.7628 - recall: 0.8820 - auc: 0.8075 - prc: 0.7482\n — val_f1: 0.3902642153698807 — val_precision: 0.25706997284472116, — val_recall: 0.8098859315589354\nEpoch 50/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1839 - f1: 0.8162 - precision: 0.7630 - recall: 0.8818 - auc: 0.8075 - prc: 0.7483A: 7s - loss: 0.1887 - f1: 0.8113 - precision: 0.\n — val_f1: 0.3346390780683647 — val_precision: 0.2060819603211147, — val_recall: 0.8895610093328724\nEpoch 51/150\n6213/6213 [==============================] - 13s 2ms/step - loss: 0.1860 - f1: 0.8140 - precision: 0.7628 - recall: 0.8821 - auc: 0.8074 - prc: 0.7481\n — val_f1: 0.38430805677688773 — val_precision: 0.2503072939822685, — val_recall: 0.8270826132042862\nEpoch 52/150\n6213/6213 [==============================] - 9s 2ms/step - loss: 0.1851 - f1: 0.8149 - precision: 0.7628 - recall: 0.8819 - auc: 0.8073 - prc: 0.7480\n — val_f1: 0.3721045601984354 — val_precision: 0.23877271169009257, — val_recall: 0.8426374006221915\nEpoch 53/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1822 - f1: 0.8178 - precision: 0.7630 - recall: 0.8816 - auc: 0.8073 - prc: 0.7481: 0s - loss: 0.1822 - f1: 0.8178 - precision: 0.7630 - recall: 0.8816 - auc: 0.8073 - prc: 0\n — val_f1: 0.40764013338475147 — val_precision: 0.27636799852357663, — val_recall: 0.776443138610439\nEpoch 54/150\n6213/6213 [==============================] - 11s 2ms/step - loss: 0.1921 - f1: 0.8079 - precision: 0.7633 - recall: 0.8811 - auc: 0.8072 - prc: 0.7482\n — val_f1: 0.3766954618301216 — val_precision: 0.24339546395918574, — val_recall: 0.8327860352575182\nEpoch 55/150\n6213/6213 [==============================] - 11s 2ms/step - loss: 0.1840 - f1: 0.8160 - precision: 0.7635 - recall: 0.8806 - auc: 0.8072 - prc: 0.7483\n — val_f1: 0.3932683790965456 — val_precision: 0.2621770314255264, — val_recall: 0.7865537504320774\nEpoch 56/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1842 - f1: 0.8158 - precision: 0.7634 - recall: 0.8807 - auc: 0.8071 - prc: 0.7482\n — val_f1: 0.37275518035303146 — val_precision: 0.23956791950281148, — val_recall: 0.8394400276529554\nEpoch 57/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1834 - f1: 0.8166 - precision: 0.7632 - recall: 0.8809 - auc: 0.8070 - prc: 0.7480\n — val_f1: 0.3991048926740245 — val_precision: 0.2665718597631762, — val_recall: 0.7937262357414449\nEpoch 58/150\n6213/6213 [==============================] - 11s 2ms/step - loss: 0.1817 - f1: 0.8184 - precision: 0.7632 - recall: 0.8810 - auc: 0.8069 - prc: 0.7479\n — val_f1: 0.39369442198868226 — val_precision: 0.26113337472484055, — val_recall: 0.7996024887659868\nEpoch 59/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1835 - f1: 0.8165 - precision: 0.7632 - recall: 0.8810 - auc: 0.8069 - prc: 0.7479A: 4s - loss: 0.1831 - f1: 0.8169 - precision: 0.7632 - recall: 0.8 - ETA: 2s - loss: 0.1832 - f1: 0.8168 - precision: 0.7632 - recall: 0.8810 - auc:  - ETA: 1s - loss: 0.1832 - f1: 0.8168 - precision: 0.7632 - recall: 0.8810 - auc: 0.806 - ETA: 0s - loss: 0.1833 - f1: 0.8167 - precision: 0.7632 - recall: 0.8810 - auc: 0.8\n — val_f1: 0.3927023273431253 — val_precision: 0.2595549374130737, — val_recall: 0.8063428966470791\nEpoch 60/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1839 - f1: 0.8161 - precision: 0.7633 - recall: 0.8808 - auc: 0.8069 - prc: 0.7478A: 7s - loss: 0.1841 - f1: 0.8158 - precision: 0.7633 - recall: 0.8808 - au - ETA: 5s - loss: 0.1841 - f1: 0.8159 - precision: 0.7633 - recall: 0.8808 - a - ETA: 4s - loss: 0.1841 - f1: 0.8159 - precision: 0.7633 - recall: 0.8808 -  - ETA: 3s - loss: 0.1841 - f1: 0.8159 - precision: 0.7633 - recall: 0.8808 - a - ETA: 2s - loss: 0.1840 - f1: 0.8160 - precision: 0.7633 \n — val_f1: 0.3648462344442198 — val_precision: 0.2317121576501903, — val_recall: 0.8575872796405116\nEpoch 61/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1845 - f1: 0.8155 - precision: 0.7631 - recall: 0.8810 - auc: 0.8068 - prc: 0.7477A: 4s - loss: 0.1851 - f1: 0.8149 - precision: 0.7631 - recall: 0.8810 - auc: 0.8 - ETA: 3s - loss: 0.1848 - f1: 0.8152 - precision: 0.7631 - recall: 0.8810 - auc: 0.8068 - p - ETA: 2s - loss: 0.1847 - f1: 0.8153 - precisi\n — val_f1: 0.36637479419248614 — val_precision: 0.23380934186646288, — val_recall: 0.8460940200483926\nEpoch 62/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1900 - f1: 0.8100 - precision: 0.7631 - recall: 0.8809 - auc: 0.8067 - prc: 0.7476\n — val_f1: 0.36805851166750514 — val_precision: 0.23712291199202193, — val_recall: 0.8218976840649844\nEpoch 63/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1906 - f1: 0.8094 - precision: 0.7628 - recall: 0.8810 - auc: 0.8064 - prc: 0.7473A: 0s - loss: 0.1907 - f1: 0.8093 - precision: 0.7628 - recall: 0.8810 - auc: 0.8064 - prc\n — val_f1: 0.40462013699243415 — val_precision: 0.27303486194187665, — val_recall: 0.7810231593501555\nEpoch 64/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1873 - f1: 0.8127 - precision: 0.7628 - recall: 0.8810 - auc: 0.8064 - prc: 0.7472A: 9s - loss: 0.1911 - f1: 0.8089 - precision: 0.7627 - recall: 0.8810 - auc: 0.8064 - ETA: 7s - loss: 0.1896 - f1: 0.8104 - precisio - ETA: 4s - loss: 0.1879 - f1: 0.8121 - precision: 0. - ETA: 2s - loss: 0.1875 - f1: 0.8125 - precision: 0.7628\n — val_f1: 0.3779434619243405 — val_precision: 0.24467028348743028, — val_recall: 0.8301071552022122\nEpoch 65/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 9s 1ms/step - loss: 0.1888 - f1: 0.8112 - precision: 0.7629 - recall: 0.8807 - auc: 0.8063 - prc: 0.7473A: 7s - loss: 0.1887 - f1: 0.8112 - precision: 0.7629 - recall: 0.8808 - auc: 0.8063 - prc: 0. - ETA: 7s - loss: 0.1889 - f1: 0.8111 - precision: 0.7629 - recall: 0.8808 - auc: 0.8063 - prc: 0.747 - ETA: 7s - loss: 0.1889 - f1: 0.8111 - precision: 0.7629 - recall: 0.8808 - auc: 0.8063 - ETA: 6s - loss: 0.1885 - f1: 0.8115 - precision: 0.7629 - recall: 0.8808 - auc: 0.8063 - prc:  - ETA: 6s - loss: 0.1887 - f1: 0.8113 - precision: 0.7629 - recall: 0.8808 - auc: 0.8063 - prc: 0.747 - ETA: 6s - loss: 0.1887 - f1: 0.8112 - precision: 0.7629 - recall: 0.88 - ETA: 4s - loss: 0.1893 - f1: 0.8107 - precision: 0.7629 - recall: 0.8807 - auc: 0.8063 - prc: - ETA: 4s - loss: 0.1892 - f1: 0.8108 - precision: 0.7629 - recall: 0.8807 - auc: - ETA: 3s - loss: 0.1891 - f1: 0.8109 - precision: 0.7629 - recall: 0.8807 - auc: 0.8063 - ETA: 2s - loss: 0.1891 - f1: 0.8109 - prec\n — val_f1: 0.372242141407243 — val_precision: 0.23847256994239324, — val_recall: 0.8478223297614933\nEpoch 66/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1850 - f1: 0.8150 - precision: 0.7631 - recall: 0.8804 - auc: 0.8063 - prc: 0.7473\n — val_f1: 0.34660572081562685 — val_precision: 0.21843633253814645, — val_recall: 0.8387487037677152\nEpoch 67/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1902 - f1: 0.8098 - precision: 0.7632 - recall: 0.8800 - auc: 0.8062 - prc: 0.7473A: 7s - loss: 0.1992 - f1: 0.8008 - precision: 0.7631 - recal - ETA: 5s - loss: 0.1944 - f1: 0.8056  - ETA: 2s - loss: 0.1914 - f1: 0.8087 - precision: 0.7631 - recall: 0.8800 - auc: 0.8062 - prc: 0 - ETA: 2s - loss: 0.1912 - f1: 0.8088 - precision: 0.7631 - recall: 0.8800 - - ETA: 1s - loss: 0.1906 - f1: 0.8094 - precision: 0.7632 - recall: 0.8800 - auc: 0. - ETA: 0s - loss: 0.1903 - f1: 0.8097 - precision: 0.7632 - recall: 0.8800 - auc: 0.8062 - prc: \n — val_f1: 0.3901486709716173 — val_precision: 0.2594960189492309, — val_recall: 0.7857760110611821\nEpoch 68/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1859 - f1: 0.8141 - precision: 0.7632 - recall: 0.8798 - auc: 0.8062 - prc: 0.7473A: 7s - loss: 0.1853 - f1: 0.8147 - precision: 0.7633 - recall: 0.8798 - auc: 0.8062 - prc: 0. - ETA: 7s - loss: 0.1854 - f1: 0.8146 - precision: 0.7633 - recall: 0.8798 - auc: 0.8062 - prc: 0. - ETA: 7s - loss: 0.1854 - f1: 0.8146 - precision: 0.7633 - recall: 0.8798 - auc: 0.8062 - pr - ETA: 6s - loss: 0.1858 - f1: 0.8142 - precision: 0.7633 - recall: 0.8798 - auc: 0.8062 - - ETA: 6s - loss: 0.1862 - f1: 0.8138 - precision: 0.7633 - recall: 0.8798 - auc: 0.8062 - pr - ETA: 5s - loss: 0.1863 - f1: 0.8137 - precision: 0.7633 - recall: 0.8798 - auc: 0.8062 - prc - ETA: 5s - loss: 0.1863 - f1: 0.8137 - precision: 0.7633 - recall: 0.8798 - auc: 0.8062 - p - ETA: 4s - loss: 0.1864 - f1: 0.8136 - precision: 0.7633 - recall: 0.8798 - auc: 0 - ETA: 3s - loss: 0.1863 - f1: 0.8137 - precision: 0.7632 - recall: 0.8798 - auc: 0.8062 - prc: 0 - ETA: 3s - loss: 0.1862 - f1: 0.8138 - precision: 0.7632 - recall: 0.8798 - auc: 0.8062 -  - ETA: 3s - loss: 0.1862 - f1: 0.8139 - precision: 0.7632 - recall: 0.8798 - auc: 0.8062 - p - ETA: 2s - loss: 0.1861 - f1: 0.8139 - precision: 0.7632 - recall: 0.8798 - auc: 0.8062 - ETA: 1s - loss: 0.1860 - f1: 0.8140 - precision: 0.7632 - recall:\n — val_f1: 0.37086449182029646 — val_precision: 0.23848493089722353, — val_recall: 0.8335637746284134\nEpoch 69/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1824 - f1: 0.8176 - precision: 0.7632 - recall: 0.8798 - auc: 0.8061 - prc: 0.7473A: 8s - loss: 0.1839 - f1: 0.8161 - precision: 0.763 - ETA: 5s - loss: 0.1811 - f1: 0.8188 - precision: 0.7632 - recall: 0.8798 - auc: 0.8061 - p - ETA: 5s - loss: 0.1814 - f1: 0.8186 - precision: 0.7632 - recall: 0.8798 - auc: 0.8061 - prc: - ETA: 5s - loss: 0.1815 - f1: 0.8185 - precisio - ETA: 2s - loss: 0.1821 - f1: 0.8178 - precision: 0.7632 - recall: 0.8798 - auc: 0.8 - ETA: 1s - loss: 0.1822 - f1: 0.8177 - precision: 0.7632 - recall: 0.8798\n — val_f1: 0.3844697363766879 — val_precision: 0.2531995943983118, — val_recall: 0.7983926719668164\nEpoch 70/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1881 - f1: 0.8119 - precision: 0.7633 - recall: 0.8797 - auc: 0.8061 - prc: 0.7473A: 8s - loss: 0.1858 - f1: 0.8141 - precision: 0.7633 - recall: 0 - ETA: 6s - loss: 0.1904 - f1: 0.8095 - precision: 0.7633 - recall: 0.8797 - auc:  - ETA: 5s - loss: 0.1902 - f1: 0.8097 - precision: 0.7633 - recall: 0.8797 - auc: 0.8061 - prc: - ETA: 5s - loss: 0.1900 - f1: 0.8099 - precision: 0.7633 - recall: 0.8797 - auc: 0.8061 - prc: 0. - ETA: 5s - loss: 0.1900 - f1: 0.8100 - precision: 0.7633 - recall: 0.8797 - auc: 0.8061 - pr - ETA: 4s - loss: 0.1897 - f1: 0.8102 - precision: - ETA: 1s - loss: 0.1886 - f1: 0.8114 - precision: 0.7633 - recall: 0.8797 - auc: 0.8061 - - ETA: 1s - loss: 0.1884 - f1: 0.8116 - precision: 0.7633 - recall: 0.8797 - auc: 0.8061 - prc: 0.747 - ETA: 1s - loss: 0.1884 - f1: 0.8116 - precision: 0.7633 - recall: 0.8797 - a\n — val_f1: 0.3724201017666228 — val_precision: 0.2388939145742158, — val_recall: 0.8443657103352921\nEpoch 71/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1868 - f1: 0.8132 - precision: 0.7634 - recall: 0.8794 - auc: 0.8061 - prc: 0.7473: 2s - loss: 0.1866 - f1: 0.81\n — val_f1: 0.39346288246825195 — val_precision: 0.2611142533936652, — val_recall: 0.7978741790528863\nEpoch 72/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1930 - f1: 0.8070 - precision: 0.7635 - recall: 0.8793 - auc: 0.8060 - prc: 0.7473A: 10s - loss: 0.1889 - f1: 0.8111 - precision: 0.7634 - recall: - ETA: 7s - lo - ETA: 1s - loss: 0.1929 - f1: 0.8071 - precision: 0.7635 - recall: 0.8793 - auc\n — val_f1: 0.3890005595043206 — val_precision: 0.25585389123620006, — val_recall: 0.8110957483581058\nEpoch 73/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1826 - f1: 0.8174 - precision: 0.7633 - recall: 0.8793 - auc: 0.8059 - prc: 0.7472A: 7s - loss: 0.1830 - f1: 0.8170 - precision: 0.7633 - recall: 0.8792 - auc: 0. - ETA: 6s - loss: 0.1828 - f1: 0.8173 - precisio - ETA: 3s - loss: 0.1821 - f1: 0.8179 - precision:  - ETA: 1s - loss: 0.1824 - f1: 0.8176 - precision: 0.7633 - recall: 0.8793 - auc: 0.8059 - prc - ETA: 0s - loss: 0.1825 - f1: 0.8175 - precision: 0.7633 - recall: 0.8793 - auc: 0.8059 \n — val_f1: 0.3776586273074412 — val_precision: 0.2449000538862231, — val_recall: 0.8247493950916004\nEpoch 74/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1893 - f1: 0.8107 - precision: 0.7633 - recall: 0.8792 - auc: 0.8058 - prc: 0.7471A: 8s - loss: 0.1865 - f1: 0.8135 - precision: 0.7633 - recall: 0.8792 - ETA: 6s - loss: 0.1885 - f1: 0.8115 - precision: 0.7633 - recall: 0.8792 - auc: 0.8058 - prc: 0 - ETA: 6s - loss: 0.1889 - f1: 0.8111 - precision: 0.7 - ETA: 4s - loss: 0.1896 - f1: 0.8104 - precision: 0. - ETA: 1s - loss: 0.1894 - f1: 0.8106 - precision: 0.7633 - recall: 0.8792 - auc:  - ETA: 0s - loss: 0.1894 - f1: 0.8106 - precision: 0.7633 - recall: 0.8792 - auc: 0.8058 - \n — val_f1: 0.38117980370332893 — val_precision: 0.24887033269032582, — val_recall: 0.8138610438990667\nEpoch 75/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1901 - f1: 0.8099 - precision: 0.7633 - recall: 0.8791 - auc: 0.8057 - prc: 0.7471A: 8s - loss: 0.1859 - f1: 0.8141 - precision: 0.7633 - recall: 0.8791 - auc - ETA: 6s - loss: 0.1884 - f1: 0.8116 - precision: 0.7633 - recall: 0.8791 - auc: 0 - ETA: 5s - loss: 0.1894 - f1: 0.8106 - precision: 0.7633 - recall: 0.8791 - auc: 0.8057 - pr - ETA: 5s - loss: 0.1897 - f1: 0.8103 - precision: 0.7633 - recall: 0.8791 - auc: 0.8057 - prc: 0.74 - ETA: 5s - loss: 0.1898 - f1: 0.8102 - preci - ETA: 2s - loss: 0.1903 - f1: 0.8097 - precision: 0.76\n — val_f1: 0.3817658078681788 — val_precision: 0.24890336476583227, — val_recall: 0.8188731420670584\nEpoch 76/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 8s 1ms/step - loss: 0.1875 - f1: 0.8125 - precision: 0.7633 - recall: 0.8790 - auc: 0.8057 - prc: 0.7470A: 5s - loss: 0.1870 - f1: 0.8130 - precision: 0.7633 - recall: 0.8790 - auc - ETA: 4s - loss: 0.1870 - f1: 0.8129 - precision: 0.7633 - recall: 0.8790 - auc: 0.8057 - prc: 0.74 - ETA: 4s - loss: 0.1870 - f1: 0.8130 - precision: 0.7633 - recall: 0.8790  - ETA: 3s - loss: 0.1870 - f1: 0.8130 - precision: 0.7633 - recall: 0.8790 - auc: 0.8057 - prc: 0.747 - ETA: 3s - loss: 0.1870 - f1: 0.8130 - precision: 0.7633 - recall: 0. - ETA: 1s - loss: 0.1872 - f1: 0.8128 - precision: 0.7633 - recall: 0.\n — val_f1: 0.37376342430424575 — val_precision: 0.24042985192888625, — val_recall: 0.8390943657103352\nEpoch 77/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1877 - f1: 0.8123 - precision: 0.7632 - recall: 0.8789 - auc: 0.8055 - prc: 0.7469\n — val_f1: 0.384595221292469 — val_precision: 0.2508218709728322, — val_recall: 0.8241444866920152\nEpoch 78/150\n6213/6213 [==============================] - 11s 2ms/step - loss: 0.1903 - f1: 0.8097 - precision: 0.7632 - recall: 0.8789 - auc: 0.8055 - prc: 0.7468: 7s - \n — val_f1: 0.3894118148132374 — val_precision: 0.25837901995139323, — val_recall: 0.7900967853439337\nEpoch 79/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1865 - f1: 0.8135 - precision: 0.7632 - recall: 0.8787 - auc: 0.8054 - prc: 0.7468\n — val_f1: 0.3845730476792303 — val_precision: 0.25164732559690545, — val_recall: 0.8151572761838921\nEpoch 80/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1900 - f1: 0.8100 - precision: 0.7632 - recall: 0.8787 - auc: 0.8053 - prc: 0.7467: 7s - loss: 0.1914 - f1: 0.8086 - precision: 0.7632 - recall: 0.8786 - auc: 0.8054 - prc: 0.74 - ETA: 7s - loss: 0.1915 - f1: 0.8085 - precision: 0.7632 - recall: 0.8786 - auc: 0.8054 - prc: 0 - ETA: 7s - loss: 0.1919 - f1: 0.8081 - precision: 0.7632 - recall: 0.8786 - a - ETA: 6s - loss: 0.1914 - f1: 0.8086 - precision: 0.7632 - recall: 0.8786 - auc: 0.805 - ETA: 5s - loss: 0.1906 - f1: 0.8094 - precision: 0.7632 - recall: 0.878\n — val_f1: 0.3681503946352214 — val_precision: 0.23538480067445502, — val_recall: 0.8444521258209471\nEpoch 81/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1904 - f1: 0.8096 - precision: 0.7628 - recall: 0.8789 - auc: 0.8052 - prc: 0.7464A: 6s - loss: 0.1909 - f1: 0.8091 - precision: 0.7628 - recall: 0 - ETA: 5s - loss: 0.1906 - f1: 0.8094 - precision: 0.7628 - recall: 0.8789 - auc: 0.8052  - ETA: 4s - loss: 0.1906 - f1: 0.8094 - precision: 0.7628 - recall: 0.8789 - a - ETA: 3s - loss: 0.1905 - f1: 0.8095 - precision: 0.7628 - recall: 0.8789 - auc: 0.8052 - prc: - ETA: 2s - loss: 0.1905 - f1: 0.8095 - p\n — val_f1: 0.4013320732640295 — val_precision: 0.27131122604198354, — val_recall: 0.770653301071552\nEpoch 82/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1910 - f1: 0.8090 - precision: 0.7627 - recall: 0.8790 - auc: 0.8050 - prc: 0.7463A: 6s - loss: 0.1941 - f1: 0.8059 - precision: 0.7627 - recall: 0.8 - ETA: 5s - loss: 0.1923 - f1: 0.8077 - precision: 0.7627 - recall: 0.8790 - auc: 0.8051 - prc: 0.74 - ETA: 5\n — val_f1: 0.36841201061332013 — val_precision: 0.23643268406223203, — val_recall: 0.8339094365710336\nEpoch 83/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1912 - f1: 0.8089 - precision: 0.7627 - recall: 0.8789 - auc: 0.8049 - prc: 0.7462\n — val_f1: 0.34683587140439925 — val_precision: 0.2156413061774112, — val_recall: 0.8856723124783962\nEpoch 84/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1851 - f1: 0.8149 - precision: 0.7625 - recall: 0.8790 - auc: 0.8048 - prc: 0.7460A: 3s - loss: 0.1850 - f1: \n — val_f1: 0.36326916306905566 — val_precision: 0.23120828321422676, — val_recall: 0.847131005876253\nEpoch 85/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1900 - f1: 0.8100 - precision: 0.7623 - recall: 0.8792 - auc: 0.8047 - prc: 0.7459A: 7s - loss: 0.1880 - f1: 0.8120 - precision: 0.7623 - reca - ETA: 5s - loss: 0.1899 - f1: 0.8101 - precision: 0.7623 - recall: 0.8792 - auc: 0.8048 - prc: 0. - ETA: 5s - loss: 0.1900 - f1: 0.8100 - precision: 0.7623 - recall: 0.8792 - auc: 0.8047 - prc - ETA: 4s - loss: 0.1901 - f1: 0.8099 - precision: 0.7623 - recall: 0.8792 - auc: 0.8047 - prc: 0.745 - ETA: 4s - loss: 0.1901 - f1: 0.8099 - precision: 0.7623 - recall: 0.8792 - auc: 0.8047 - prc: 0.745 - ETA: 4s - loss: 0.1901 - f1: 0.8099 - precision: 0.7623 - recall: 0.8792 - auc: 0.8047 - prc: 0.74 - ETA: 4s - loss: 0.1901 - f1: 0.8099 - precision: 0.7623 - recall:  - ETA: 2s - loss: 0.1901 - f1: 0.8099 - precision: 0.7623 - recall: 0.8792 - auc: 0.8047 - prc: 0.7 - ETA: 2s - loss: 0.1901 - f1: 0.8099 - precision: 0.7623 - recall: 0.8792 - auc: 0.8047 - - ETA: 2s - loss: 0.1900 - f1: 0.8100 - precision: 0.7623 - recall: 0.8792 - auc: 0.8047 -  - ETA: 1s - loss: 0.1900 - f1: 0.8100 - precision: 0.7623 - recall: 0.8792 - auc: 0.8047 - prc: 0.74 - ETA: 1s - loss: 0.1900 - f1: 0.8100 - precision: 0.7623 - recall: 0.8792 - auc: 0.8047 - prc: - ETA: 1s - loss: 0.1900 - f1: 0.8100 - precision: 0.7623 - recall: 0.8792 - au\n — val_f1: 0.3610104297655152 — val_precision: 0.22964897421419161, — val_recall: 0.8435015554787418\nEpoch 86/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1917 - f1: 0.8083 - precision: 0.7622 - recall: 0.8792 - auc: 0.8046 - prc: 0.7457A: 7s - loss: 0.1949 - f1: 0.8052 - precision: 0.7622 - recall: 0.8792 - auc: 0.8046 - pr - ETA: 7s - loss: 0.1942 - f1: 0.8058 - precision: 0.7622 - recall: 0.8792 - auc: 0.8046 - ETA: 6s - loss: 0.1938 - f1: 0.8062 - precision:  - ETA: 3s - loss: 0.1927 - f1: 0.8073 - precision: 0.7622 - recall: 0.8792 - auc: 0.8046 - prc: 0. - ETA: 3s - loss: 0.1927 - f1: 0.8074 - precision: 0.7622 - recall: 0.8792 - auc: 0.8046 - prc: - ETA: 3s - loss: 0.1925 - f1: 0.8075 - precision: 0.7622 - recall: 0.8792 - auc: 0.8046 - prc: - ETA: 3s - loss: 0.1924 - f1: 0.8076 - precision: 0.7622 - recall: 0.8792 - auc: 0. - ETA: 2s - loss: 0.1922 - f1: 0.8078 - precision: 0.7622 - recall: 0.8792 - auc: 0.8046 - prc:  - ETA: 1s - loss: 0.1921 - f1: 0.8079 - precision: 0.7622 - recall: 0.8792 - auc: 0.8046 - prc:  - ETA: 1s - loss: 0.1920 - f1: 0.8080 - precision: 0.7622 - recall: 0.8792 - auc: 0.8046 - prc: 0.74 - ETA: 1s - loss: 0.1920 - f1: 0.8080 - precision: 0.7622 - recall: 0.8792 - au - ETA: 0s - loss: 0.1918 - f1: 0.8083 - precision: 0.7622 - recall: 0.8792 - auc: 0.8046 - prc:\n — val_f1: 0.3931772691671743 — val_precision: 0.26273551988834615, — val_recall: 0.7808503283788455\nEpoch 87/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1932 - f1: 0.8068 - precision: 0.7622 - recall: 0.8790 - auc: 0.8046 - prc: 0.7457A: 8s - loss: 0.1900 - f1: 0.8101 - precision: 0.7622 - recall: 0.8791 - auc: 0.8046 - pr - ETA: 7s - loss: 0.1908 - f1: 0.8092 - precision: 0.7622 - recall: 0.8791 - auc: 0.8046 - - ETA: 6s - loss: 0.1919 - f1: 0.8081 - precision: 0.762 - ETA: 4s - loss: 0.1929 - f1: 0.8071 - precision: 0.7622 - recall: 0.8 - ETA: 2s - loss: 0.1931 - f1: 0.8069 - precision: 0.7622 - recall: 0.8790 - auc: 0.8046 - prc: 0.7 - ETA: 2s - loss: 0.1932 - f1: 0.8068 - precision: 0.7622 - recall: 0.8790 - auc: 0.8046 - p - ETA: 2s - loss: 0.1932 - f1: 0.8068 - precision: 0.7622 - \n — val_f1: 0.39794292995949576 — val_precision: 0.2700932847346636, — val_recall: 0.7556170065675769\nEpoch 88/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 9s 1ms/step - loss: 0.1878 - f1: 0.8122 - precision: 0.7624 - recall: 0.8786 - auc: 0.8045 - prc: 0.7458A: 9s - loss: 0.1896 - f1: 0.8104 - precision: 0.7624 - recall: 0.8786 - auc: 0.8046 - prc: 0. - ETA: 8s - loss: 0.1903 - f1: 0.8097 - precision: 0.7624 - recall: 0.8786 - auc: 0.8046 - p - ETA: 8s - loss: 0.1898 - f1: 0.8102 - precision: 0.7624 - recall: 0.8786 - auc: 0.8046  - ETA: 7s - loss: 0.1886 - f1: 0.8114 - precision: 0.7624 - recall: 0.8786 - auc: 0.8 - ETA: 6s - loss: 0.1879 - f1: 0.8121 - precision: 0.7624 - recall: 0.8786  - ETA: 5s - loss: 0.1876 - f1: 0.8124 - precision: 0.762 - ETA: 2s - loss: 0.1879 - f1: 0.8121 - precision: 0.76\n — val_f1: 0.35625380317530564 — val_precision: 0.22644693968447455, — val_recall: 0.8347735914275838\nEpoch 89/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1891 - f1: 0.8109 - precision: 0.7623 - recall: 0.8786 - auc: 0.8045 - prc: 0.7457A: 6s - loss: 0.1910 - f1: 0.8090 - precision: 0.7623 - recall: 0.8787 - auc: 0.8045 - prc: 0. - ETA: 6s - loss: 0.1911 - f1: 0.8089 - precision: 0.7623 - recall: 0.8787 - auc: 0.8045 - prc: 0. - ETA: 6s - loss: 0.1912 - f1: 0.8088 - precision: 0.7623 - recall: 0.8787 - auc: 0.8045 - prc: 0.7 - ETA: 6s - loss: 0.1912 - f1: 0.8088 - precision: 0.7623 - recall: 0.8787 - auc: 0.8045 - pr - ETA: 5s - loss: 0.1911 - f1: 0.8089 - precision: 0.7623 - recall: 0.8787 - auc: 0.8045 - prc: - ETA: 5s - loss: 0.1910 - f1: 0.8090 - precision: 0.7623 - recall: 0.8787 - auc: 0.8045 - prc: 0.745 - ETA: 5s - loss: 0.1910 - f1: 0.8090 - precision: 0.7623 - rec - ETA: 3s - loss: 0.1899 - f1: 0.8101 - precision: 0.7623 - recall: 0.8786 - auc: 0.8045 - prc - ETA: 3s - loss: 0.1898 - f1: 0.8103 - precision: 0.7623 - recall: 0.8786 - - ETA: 1s - loss: 0.1894 - f1: 0.8107 - precision: 0.7623 - recall: 0.8786 - auc: 0.8045 - prc: 0. - ETA: 1s - loss: 0.1893 - f1: 0.8107 - precision: 0.7623 - recall: 0.8786 - auc: 0.8045 - prc: 0 - ETA: 1s - loss: 0.1893 - f1: 0.8107 - precision: 0.7623 - recall: 0.8786 - auc: 0.8045 - prc - ETA: 0s - loss: 0.1892 - f1: 0.8108 - precision: 0.7623 - recall: 0.8786 - auc: 0.80 - ETA: 0s - loss: 0.1891 - f1: 0.8109 - precision: 0.7623 - recall: 0.8786 - auc: 0.8045 - prc: 0.7\n — val_f1: 0.3987473903966598 — val_precision: 0.26833861421653615, — val_recall: 0.7757518147251987\nEpoch 90/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1899 - f1: 0.8101 - precision: 0.7623 - recall: 0.8785 - auc: 0.8044 - prc: 0.7456A: 7s - loss: 0.1935 - f1: 0.8065 - precision: 0.7623 - recall: 0.8785 - auc: 0.8 - ETA: 6s - loss: 0.1926 - f1: 0.8074 - precision: 0.7623 - reca - ETA: 4s - loss: 0.1912 - f1: 0.8088 - precision: 0.7 - ETA: 1s - loss: 0.1902 - f1: 0.8098 - precision: 0.7623 - recall: 0.8785 - auc: 0.8044 - - ETA: 1s - loss: 0.1901 - f1: 0.8099 - precision: 0.7623 - recall: 0.8785 - \n — val_f1: 0.4069406477320113 — val_precision: 0.2779958219915174, — val_recall: 0.758987210508123\nEpoch 91/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1881 - f1: 0.8119 - precision: 0.7624 - recall: 0.8784 - auc: 0.8044 - prc: 0.7456A: 8s - loss: 0.1860 - f1: 0.8138 - precision: 0.7623 - recall: 0.8784 - auc: 0.8044 - prc: 0.745 - ETA: 8s - loss: 0.1854 - f1: 0.8145 - precision: 0.7623 - recall: - ETA: 6s - loss: 0.1853 - f1: 0.8146 - precision: 0.7623 - recall: 0.8784 - auc: 0.8044 - prc: 0.745 - ETA: 6s - loss: 0.1854 - f1: 0.8146 - precision: 0.7623 - recall: 0.8784 - ETA: 5s - loss: 0.1855 - f1: 0.8145 - precision: 0.7623 - recall: 0.8784 - auc: 0 - ETA: 4s - loss: 0.1856 - f1: 0.8144 - precision: 0.7623 - reca - ETA: 2s - loss: 0.1867 - f1: 0.8133 - precision: 0.76\n — val_f1: 0.3903660104187809 — val_precision: 0.2641054590949315, — val_recall: 0.7479260283442793\nEpoch 92/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1980 - f1: 0.8020 - precision: 0.7626 - recall: 0.8778 - auc: 0.8044 - prc: 0.7458\n — val_f1: 0.4079511132963405 — val_precision: 0.279777770641318, — val_recall: 0.752851711026616\nEpoch 93/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1918 - f1: 0.8082 - precision: 0.7628 - recall: 0.8774 - auc: 0.8043 - prc: 0.7458A: 4s - loss: 0\n — val_f1: 0.37748743718592964 — val_precision: 0.24595316674524595, — val_recall: 0.8114414103007259\nEpoch 94/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1940 - f1: 0.8060 - precision: 0.7627 - recall: 0.8773 - auc: 0.8042 - prc: 0.7458A: 8s - loss: 0.1899 - f1: 0.8101 - precision: 0.7627 - recall: 0.8774 - auc: - ETA: 7s - loss: 0.1957 - f1: 0.8043 - precision: 0.7627 - recall: 0.8774 - auc: 0.8042 - prc: 0 - ETA: 7s - loss: 0.1971 - f1: 0.8029 - precision: 0.7627 - recall: 0.8774 - auc: 0.8042 - pr - ETA: 6s - loss: 0.1985 - f1: 0.8015 - precision: 0.7627 - recall: 0.8774 - auc: 0.8042 -  - ETA: 6s - loss: 0.1989 - f1: 0.8011 - precision: 0.7627 - recall: 0.8774 - auc: 0.8042 - prc: 0.745 - ETA: 6s - loss: 0.1989 - f1: 0.8011 - precision: 0.7627 - recall: 0.8774 - auc: 0.8042  - ETA: 5s - loss: 0.1985 - f1: 0.8015 - precision: 0.7627 - recall: 0.8774 - auc: 0.8042 - prc:  - ETA: 5s - loss: 0.1982 - f1: 0.8018 - precision: 0.7627 - recall: 0.8774 - auc: 0.8 - ETA: 4s - loss\n — val_f1: 0.39069457738565905 — val_precision: 0.2573041714630269, — val_recall: 0.8112685793294159\nEpoch 95/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1865 - f1: 0.8135 - precision: 0.7626 - recall: 0.8774 - auc: 0.8042 - prc: 0.7456A: 2s - loss: 0.1854 - f1: 0.8146 - precision: 0\n — val_f1: 0.4091610792378056 — val_precision: 0.2789594395048541, — val_recall: 0.7672830971310058\nEpoch 96/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1907 - f1: 0.8094 - precision: 0.7628 - recall: 0.8771 - auc: 0.8042 - prc: 0.7457A: 8s - loss: 0.1916 - f1: 0.8084 - precision: 0.7628 - recall:  - ETA: 6s - loss: 0.1926 - f1: 0.8074 - precision: 0.7628 - recall: 0.8771  - ETA: 4s - loss: 0.1922 - f1: 0.8078 - precision: 0.7628 - recall: 0.877 - ETA: 3s - loss: 0.1918 - f1: 0.8082 - precision: 0.7628 - recall: 0.8771 - auc: 0.8042 - prc: 0. - ETA: 3s - loss: 0.1917 - f1: 0.8083 - precision: 0.7628 - recall: 0.8771 - auc: 0.8042 - p - ETA: 2s - loss: 0.1916 - f1: 0.8084 - precision: \n — val_f1: 0.3916325299952174 — val_precision: 0.2578657685040664, — val_recall: 0.8137746284134116\nEpoch 97/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1872 - f1: 0.8128 - precision: 0.7627 - recall: 0.8772 - auc: 0.8041 - prc: 0.7456A: 8s - loss: 0.1876 - f1: 0.8123 - precision: 0.7627 - recall: 0.8772 - auc: 0.8041 -  - ETA: 8s - loss: 0.1871 - f1: 0.8129 - precision: 0.7627 - recall: 0.8 - ETA: 6s - loss: 0.1889 - f1: 0.8110 - precision: 0.7627 - recall: 0.8772 - auc: 0.8041 - - ETA: 5s - loss: 0.1888 - f1: 0.8112 - precision: 0.7626 - recall: 0.8772 - auc: 0.8041 - ETA: 4s - loss: 0.1885 - f1: 0.8115 - precision: 0.7626 - recall: 0.8772 - auc: 0.8041 - prc - ETA: 4s - loss: 0.1883 - f1: 0.8117 - precision: 0.7626 - recall: 0.8772 - auc: 0.8041  - ETA: 3s - loss: 0.1880 - f1: 0.8120 - precision: 0.7626 - recall: 0.8772 - auc: 0.8 - ETA: 3s - loss: 0.1878 - f1: 0.8122 - precision: 0.7627 - recall: 0.8772 - auc: 0.8041 - prc: 0.74 - ETA: 2s - loss: 0.1877 - f1: 0.8122 - precision: 0.7627 - recall: 0.8772 - auc: 0.8041 - p - ETA: 2s - loss: 0.1876 - f1: 0.8123 - precision: 0.7627 - recall: 0.8772 - auc: 0.8041 - prc:  - ETA: 2s - loss: 0.1876 - f1: 0.8124 - precision: 0.7627 - recall: 0.8772 - auc: 0.8041 - prc: - ETA: 1s - loss: 0.1875 - f1: 0.8125 - precision: 0.7627 - recall: 0.877 - ETA: 0s - loss: 0.1873 - f1: 0.8127 - precision: 0.7627 - recall: 0.8772 - auc: 0.8041 - pr\n — val_f1: 0.3540531485302157 — val_precision: 0.22241176861554823, — val_recall: 0.86752506049084\nEpoch 98/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 8s 1ms/step - loss: 0.1879 - f1: 0.8121 - precision: 0.7626 - recall: 0.8773 - auc: 0.8040 - prc: 0.7456A: 7s - loss: 0.1853 - f1: 0.8146 - precision: 0.7626 - recall: - ETA: 5s - loss: 0.1857 - f1: 0.8143 - precision: 0.7626 - recall: 0.8773 - auc: 0.8040 - prc: 0. - ETA: 5s - loss: 0.1860 - f1: 0.8140 - precision: 0.7626 - recall: 0.8773 - auc: 0.8040 - prc: - ETA: 5s - loss: 0.1865 - f1: 0.81 - ETA: 1s - loss: 0.1877 - f1: 0.8123 - precision: 0.7626 - recall: 0.8773 - auc: 0.8040 - - ETA: 1s - loss: 0.1878 - f1: 0.8122 - precision: 0.7626 - recall: 0.8773 - auc\n — val_f1: 0.3646583654400952 — val_precision: 0.2323805005337445, — val_recall: 0.8465260974766678\nEpoch 99/150\n6213/6213 [==============================] - ETA: 0s - loss: 0.1876 - f1: 0.8124 - precision: 0.7626 - recall: 0.8772 - auc: 0.8040 - prc: 0.7456- ETA: 7s - loss: 0.1874 - f1: 0.8125 - precision: 0.7627 - recall: 0.8772 - au - ETA: 6s - loss: 0.1875 - f1: 0.8125 - precision: 0.7626 - recall: 0.8772 - auc: 0.8040 - prc:  - ETA: 6s - loss: 0.1874 - f1: 0.8126 - precision: 0.7626 - recall: 0.8772 - auc: 0.8040 - prc: 0.745 - ETA: 6s - loss: 0.1874 - f1: 0.8126 - precision: 0.7626 - recall: 0.8772 - auc: 0.8040 - prc: 0. - ETA: 5s - loss: 0.1873 - f1: 0.8127 - precision: 0.7626 - recall: 0.8772 - auc: 0.8040 - prc: 0. - ETA: 5s - loss: 0.1873 - f1: 0.8127 - precision: 0.7626 - recall: 0.8772 - auc: 0.8040 - p - ETA: 5s - loss: 0.1873 - f1: 0.8127 - precision: 0.7626 - recall: 0.8772 - auc: 0.8040 - prc: 0 - ETA: 5s  - 9s 1ms/step - loss: 0.1876 - f1: 0.8124 - precision: 0.7626 - recall: 0.8772 - auc: 0.8040 - prc: 0.7456\n — val_f1: 0.35730908697299346 — val_precision: 0.2257917417177774, — val_recall: 0.8557725544417559\nEpoch 100/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1853 - f1: 0.8147 - precision: 0.7626 - recall: 0.8772 - auc: 0.8040 - prc: 0.7455A: 8s - loss: 0.1932 - f1: 0.8068 - precisio\n — val_f1: 0.38592198364633673 — val_precision: 0.25255544583522377, — val_recall: 0.817749740753543\nEpoch 101/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1830 - f1: 0.8170 - precision: 0.7626 - recall: 0.8772 - auc: 0.8040 - prc: 0.7455A: 2s - loss: 0.1824 - f1: 0.8176 - precision:\n — val_f1: 0.38817960120499206 — val_precision: 0.2544257891202149, — val_recall: 0.8184410646387833\nEpoch 102/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1826 - f1: 0.8174 - precision: 0.7627 - recall: 0.8771 - auc: 0.8040 - prc: 0.7456A: 7s - loss: 0.1809 - f1: 0.8191 - precision: 0.7627 - recall: 0.8771 - auc: 0.8040 - prc: 0.74 - ETA: 7s - loss: 0.1810 - f1: 0.8191 - precision: 0.7627 - recall: 0.8771 - auc: - ETA: 6s - loss: 0.1813 - f1: 0.8187 - precision: 0.762 - ETA: 4s - loss: 0.1820 - f1: 0.8180 - precision: 0.7627 - recall: 0.8771 - auc: 0.8040 - - ETA: 3s - loss: 0.1821 - f1: 0.8179 - precision: 0.7627 - recall: 0.8771 - a - ETA: 2s - loss: 0.1823 - f1: 0.8177 - precision: 0.7627 - recall: 0.8771 - auc: 0 - ETA: 1s - loss: 0.1824 - f1: 0.8176 - precision: 0.7627 - recall: 0.8771 - auc: 0 - ETA: 0s - loss: 0.1825 - f1: 0.8175 - precision: 0.7627 - recall: 0.8771 - auc: 0.8040 - prc:\n — val_f1: 0.3996428898452523 — val_precision: 0.27182984554060785, — val_recall: 0.7543207742827515\nEpoch 103/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1936 - f1: 0.8064 - precision: 0.7628 - recall: 0.8770 - auc: 0.8040 - prc: 0.7456A: 8s - loss: 0.1941 - f1: 0.8059 - precision: 0.7628 - recall: 0.8771 - auc: 0.8 - ETA: 7s - loss: 0.1940 - f1: 0.8060 - precision: 0.7628 - recall: 0.8771 - auc: 0.8040 - prc: 0. - ETA: 7s - loss: 0.1943 - f1: 0.8057 - precision: 0.7628 - recall: 0.8771 - auc: - ETA: 5s - loss: 0.1944 - f1: 0.8056 - precision: 0.7628 - recall: 0.8770 - auc: 0.8040 - prc: 0.745 - ETA: 5s - loss: 0.1943 - f1: 0.8057 - precision: 0.7628 - recall: 0.8770 - auc: 0.8040 - prc: 0.745 - ETA: 5s - loss: 0.1943 - f1: 0.8057 - precision: 0.7628 - recall: 0.87 - ETA: 4s - loss: 0.1928 - f1: 0.8072 - precision: 0.7628 - recall: 0.8770 - auc: 0.8040 - prc: 0. - ETA: 4s - loss: 0\n — val_f1: 0.40158555976082233 — val_precision: 0.27103171004504095, — val_recall: 0.7748012443829935\nEpoch 104/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1912 - f1: 0.8088 - precision: 0.7630 - recall: 0.8766 - auc: 0.8040 - prc: 0.7457A: 7s - loss: 0.1910 - f1: 0.8089 - precision: 0.7630 - recall: 0. - ETA: 6s - loss: 0.1916 - f1: 0.8084 - precision: 0.763 - ETA: 3s - loss: 0.1916 - f1: 0\n — val_f1: 0.3724198506807203 — val_precision: 0.2390254663104488, — val_recall: 0.8427238161078465\nEpoch 105/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1847 - f1: 0.8153 - precision: 0.7629 - recall: 0.8765 - auc: 0.8040 - prc: 0.7456A: 8s  - ETA: 3s - loss: 0.1843 - f1: 0.8157 - precision: 0.7630 - recall: 0.8765 - auc: 0. - ETA: 2s - loss: 0.1844 - f1: 0.8156 - precision: 0.7630 - recall: 0.8765 - auc: 0 - ETA: 1s - loss: 0.1845 - f1: 0.8155 - precision: 0.7630 - recall: 0.8765 \n — val_f1: 0.3776007516000862 — val_precision: 0.24408512361142742, — val_recall: 0.8335637746284134\nEpoch 106/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1866 - f1: 0.8135 - precision: 0.7629 - recall: 0.8766 - auc: 0.8039 - prc: 0.7456A: 9s - loss: 0.1823 - f1: 0.8178 - precision: 0.7629 - recall: 0.8766 - a - ETA: 7s - loss: 0.1878 - f1: 0.8122 - precision: 0.7629 - recall - E\n — val_f1: 0.38851493479175425 — val_precision: 0.2567560053380783, — val_recall: 0.7980470100241963\nEpoch 107/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1904 - f1: 0.8096 - precision: 0.7629 - recall: 0.8765 - auc: 0.8039 - prc: 0.7456A: 7s - loss: 0.1914 - f1: 0.8086 - precision: 0.7629 - recall: 0.8765 - auc: 0.8039 - prc: 0.745 - ETA: 7s - loss: 0.1914 - f1: 0.8086 - precision: 0.7629 - recall: 0.8765 - auc: 0.8039 - ETA: 6s - loss: 0.1911 - f1: 0.8088 - precision: 0.7629 - recall: 0.8765 - auc: 0.8039 - prc: 0.745 - ETA: 6s - loss: 0.1911 - f1: 0.8089 - precision: 0.7629 - recal - ETA: 5s - loss: 0.1908 - f1: 0.80 - ETA: 2s - loss: 0.1906 - f1: 0.8094 - precision: 0.7629 - recall: 0.8765 - - ETA: 0s - loss: 0.1905 - f1: 0.8095 - precision: 0.7629 - recall: 0.8765 - auc: 0.803\n — val_f1: 0.39704096775644193 — val_precision: 0.268342350009118, — val_recall: 0.7629623228482544\nEpoch 108/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1948 - f1: 0.8052 - precision: 0.7630 - recall: 0.8763 - auc: 0.8038 - prc: 0.7456A: 9s - loss: 0.2007 - f1: 0.7994 - precision: 0.7630 - recall: 0.8764 - auc: 0.8039 - prc: 0.7456 - ETA: 9s - loss: 0.1996 - f1: 0.8004 - precision: 0 - ETA: 5s - loss: 0.1989 - f1: 0.8011 - precision: 0.7630 - recall: 0.8764 - auc: 0.8039 - prc: 0.74 - ETA: 5s - loss: 0.1989 - f1: 0.8011 - precision: 0.7630 - recall: 0.8764 - auc: 0.8039 - prc: 0.745 - ETA: 5s - loss: 0.1988 - f1: 0.8012 - precision: 0.7630 - recall: 0.8764 - auc: 0.8039 - prc:  - ETA: 5s - loss: 0.1985 - f1: 0.8015 - precision: 0.7630 - recall: 0.8764 - auc: 0.8039 - prc: 0.745 - ETA: 5s - loss: 0.1985 - f1: 0.8015 - precision: 0.7630 - recall: 0.8764 - auc: 0.8039 - prc - ETA: 5s - loss: 0.1981 - f1: 0.8019 - precision: 0.7630 - recall: 0.8764 - - ETA: 3s - loss: 0.1969 - f1: 0.8031 - precision: 0.7630 - recall: 0.8764 - auc: 0.8038 - prc: - ETA: 3s - loss: 0.1966 - f1: 0.8034 - precision: 0.7630 - recall: 0.8 - ETA: 1s - loss: 0.1956 - f1: 0.8044 - precision: 0.7630 - recall: 0.87 - ETA: 0s - loss: 0.1949 - f1: 0.8051 - precision: 0.7630 - recall: 0.8763 - auc: 0.8038 - prc: \n — val_f1: 0.3510803012175382 — val_precision: 0.2204179922234005, — val_recall: 0.8621673003802282\nEpoch 109/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1895 - f1: 0.8105 - precision: 0.7629 - recall: 0.8764 - auc: 0.8037 - prc: 0.7455 ETA: 2s - loss: 0.1899 - f1: 0.8101 - precision: 0.7628 - recall: 0 - ETA: 1s - loss: 0.1896 - f1: 0.8104 - precision: 0.7629 - recall: 0.8764 - auc\n — val_f1: 0.403986651032741 — val_precision: 0.2733097388332926, — val_recall: 0.7741099204977532\nEpoch 110/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 8s 1ms/step - loss: 0.1872 - f1: 0.8127 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0.7455A: 8s - loss: 0.1872 - f1: 0.8128 - precision: 0.7629 - recall: 0.8763 - auc: 0.8038 - prc: 0. - ETA: 7s - loss: 0.1867 - f1: 0.8132 - precision: 0.7629 -  - ETA: 5s - loss: 0.1861 - f1: 0.8138 - precision: 0.7629 - recall: 0.8762 - auc: - ETA: 4s - loss: 0.1864 - f1: 0.8136 - precision: 0.7629 - recall: 0.8762 - auc: 0.8 - ETA: 3s - loss: 0.1867 - f1: 0.8132 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0.745 - ETA: 3s - loss: 0.1868 - f1: 0.8132 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0 - ETA: 3s - loss: 0.1868 - f1: 0.8131 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0.74 - ETA: 3s - loss: 0.1869 - f1: 0.8131 - precision: 0.7629 - recall: 0.8762 - au - ETA: 2s - loss: 0.1872 - f1: 0.8128 - precision: 0.7629 - recall:  - ETA: 0s - loss: 0.1872 - f1: 0.8127 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0.7 - ETA: 0s - loss: 0.1872 - f1: 0.8127 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0.\n — val_f1: 0.3585233480807466 — val_precision: 0.2261337189597126, — val_recall: 0.8648461804355341\nEpoch 111/150\n6213/6213 [==============================] - ETA: 0s - loss: 0.1851 - f1: 0.8149 - precision: 0.7630 - recall: 0.8761 - auc: 0.8037 - prc: 0.7456- ETA: 6s - loss: 0.1837 - f1: 0.8163 - precision: 0.7630 - recall: 0.876 - ETA: 4s - loss: 0.1843 - f1: 0.8157 - precision: 0.7630 - recall: 0.8761 - auc: 0.8038 - prc: 0.7 - ETA: 4s - loss: 0.1844 - f1: 0.8156 - precision: 0.7630 - recall: 0.8761 - auc: 0.8038 - prc:  - ETA: 4s - loss: 0.1845 - f1: 0.8156 - precision: 0.7630 - recall: 0.8761 - a - ETA: 2s - loss: 0.1847 - f1: 0.8153 - precision: 0.7630 - recall: 0.8761 -  - ETA: 1s - loss: 0.1849 - f1: 0.8151 - precision: 0.7630 - recall: - 9s 1ms/step - loss: 0.1851 - f1: 0.8149 - precision: 0.7630 - recall: 0.8761 - auc: 0.8037 - prc: 0.7456\n — val_f1: 0.3591265087994103 — val_precision: 0.22823413674372847, — val_recall: 0.8420324922226063\nEpoch 112/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1859 - f1: 0.8141 - precision: 0.7630 - recall: 0.8761 - auc: 0.8037 - prc: 0.7455\n — val_f1: 0.37601052102270527 — val_precision: 0.24221358449195196, — val_recall: 0.8400449360525406\nEpoch 113/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1835 - f1: 0.8165 - precision: 0.7630 - recall: 0.8761 - auc: 0.8037 - prc: 0.7456\n — val_f1: 0.3865726238354115 — val_precision: 0.2522883747922657, — val_recall: 0.826477704804701\nEpoch 114/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1822 - f1: 0.8178 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0.7455A: 7s - loss: 0.1764 - f1: 0.8236 - precision: 0.7629 - recall: 0.8762 - a - ETA: 6s - loss: 0.1788 - f1: 0.8212 - precision: 0.762 - ETA: 3s - loss: 0.1808 - f1: 0.8192 - precision:  - ETA: 1s - loss: 0.1819 - f1: 0.8182 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0. - ETA: 1s - loss: 0.1819 - f1: 0.8181 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0 - ETA: 0s - loss: 0.1820 - f1: 0.8180 - precision: 0.7629 - recall: 0.8762 - auc: 0.8\n — val_f1: 0.3859415341051053 — val_precision: 0.2542242703533026, — val_recall: 0.8008987210508123\nEpoch 115/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1901 - f1: 0.8099 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0.7455A: 8s - loss: 0.1900 - f1: 0.8101 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - ETA: 7s - loss: 0.1891 - f1: 0.8109 - precision: 0.7629 - recall: 0.8762 - auc:  - ETA: 6s - loss: 0.1880 - f1: 0.8121 - precision: 0 - ETA: 3s - loss: 0.1898 - f1: 0.8103 - precision: 0.7629 - recall: 0.8762 - - ETA: 2s - loss: 0.1902 - f1: 0.8098 - precision: 0.7629 - recall: 0.8762 - auc: 0.8037 - prc: 0.7 - ETA: 2s - loss: 0.1902 - f1: 0.8098 - precision: 0.7629 - recall: 0.8762 - auc:  - ETA: 1s - loss: 0.1902 - f1: 0.8098 - precision: 0.7629 - recall: 0.8762 - \n — val_f1: 0.4052861442375145 — val_precision: 0.274533247493792, — val_recall: 0.7738506740407881\nEpoch 116/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1852 - f1: 0.8148 - precision: 0.7631 - recall: 0.8759 - auc: 0.8037 - prc: 0.7456A: 7s - loss: 0.1866 - f1: 0.8134 - precision: 0.7631 - recall: 0.8759 - - ETA: 0s - loss: 0.1851 - f1: 0.8149 - precision: 0.7631 - recall: 0.8759 - auc: 0.8037 - prc:\n — val_f1: 0.3476321936716925 — val_precision: 0.21857753249065337, — val_recall: 0.8487729001036985\nEpoch 117/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1856 - f1: 0.8145 - precision: 0.7630 - recall: 0.8760 - auc: 0.8037 - prc: 0.7455A: 6s - loss: 0.1870 - f1: 0.8130 - precision: 0.7631 - recall: 0.8760 - auc: 0.8037 - prc: 0.745 - ETA: 6s - loss: 0.1870 - f1: 0.8130 - precision: 0.7631 - recall:  - ETA: 4s - loss: \n — val_f1: 0.40201729106628237 — val_precision: 0.2718304713189624, — val_recall: 0.7715174559281023\nEpoch 118/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1881 - f1: 0.8119 - precision: 0.7629 - recall: 0.8761 - auc: 0.8036 - prc: 0.7454A: 7s - loss: 0.1893 - f1: 0.8106 - precision: 0.7630 - recall: 0.8761 - auc: 0.8036 - - ETA: 6s - loss: 0.1891 - f1: 0.8109 - precision: 0.7630 - recall: 0.8761 - a - ETA: 5s - loss: 0.1889 - f1: 0.8110 - precision: 0.7630 - recall: 0.8761 - auc: 0.8 - ETA: 4s - loss: 0.1888 - f1: 0.8112 - precision: 0.7630 - recall: 0.8761 - ETA: 2s - loss: 0.1885 - f1: 0.8115 - precision: 0.7629 - recall: 0.8761 - auc: 0.8036 - prc: 0 - ETA: 2s - loss: 0.1885 - f1: 0.8115 - precisio\n — val_f1: 0.39912590198265463 — val_precision: 0.27092926233814396, — val_recall: 0.7576045627376425\nEpoch 119/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1873 - f1: 0.8127 - precision: 0.7629 - recall: 0.8761 - auc: 0.8036 - prc: 0.7454A: 6s - loss: 0.1895 - f1: 0.8104 - precision: 0.7629 - recall: 0.8761 - auc: 0.8036 - prc:  - ETA: 6s - loss: 0.1892 - f1: 0.8108 - precision: 0.7629 - recall: 0.8761 - auc: 0 - ETA: 5s - loss: 0.1882 - f1: 0.8118 - precision: 0.7629 - recall: 0.8761 - auc - ETA: 4s - loss: 0.1875 - f1: 0.8125 - precision: 0.7629 - recall: 0.8761 - auc: 0.8036 - prc - ETA: 4s - loss: 0.1875 - f - ETA: 0s - loss: 0.1874 - f1: 0.8126 - precision: 0.7629 - recall: 0.8761 - auc: 0.8036 - pr\n — val_f1: 0.38039640507141703 — val_precision: 0.24770090918591284, — val_recall: 0.8193052194953335\nEpoch 120/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1860 - f1: 0.8140 - precision: 0.7628 - recall: 0.8762 - auc: 0.8035 - prc: 0.7452A: 8s - loss: 0.1844 - f1: 0.8156 - precision: 0.7628 - recall: 0.8762 - auc: 0.8035 - ETA: 7s - loss: 0.1837 - f1: 0.8164 - precision: 0.7628 - recall: 0.8762 - auc: 0.8035 - prc: 0.7 - ETA: 7s - loss: 0.1837 - f1: 0.8163 - precision: 0.7628 - recall: 0.8762 - auc: 0.8035 - prc: 0.745 - ETA: 7s - loss: 0.1838 - f1: 0.8163 - precision: 0.7628 - recall: 0.8762 - auc: 0.8035 - prc - ETA: 7s - loss: 0.1842 - f1: 0.8159 - precision: 0.7628 - recall: 0.8762 - auc: 0.8035 - prc:  - ETA: 6s - loss: 0.1845 - f1: 0.8156 - precision: 0.7628 - recall: 0.8762 - auc: 0.8035 -  - ETA: 6s - loss: 0.1850 - f1: 0.8150 - precision - ETA: 3s - loss: 0.1860 - f1: 0.8141 - precision: 0.7628 - recall: 0.8762 - auc: 0.8035 - prc: 0 - ETA: 3s - loss: 0.1860 - f1: 0.8140 - precision: 0.7 - ETA: 0s - loss: 0.1860 - f1: 0.8140 - precision: 0.7628 - recall: 0.8762 - auc: 0.8\n — val_f1: 0.369337048204267 — val_precision: 0.23700710262062208, — val_recall: 0.8362426546837193\nEpoch 121/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 9s 1ms/step - loss: 0.1843 - f1: 0.8157 - precision: 0.7627 - recall: 0.8763 - auc: 0.8035 - prc: 0.7452A: 7s - loss: 0.1842 - f1: 0.8158 - precision: 0.7627 - - ETA: 5s - loss: 0.1838 - f1: 0.8162 - precision: 0.7627 - recall: 0.8763 - auc: 0.8035 - prc: 0.74 - ETA: 4s - loss: 0.1838 - f1: 0.8162 - precision: 0.7627 - recall: 0.8763 - auc: 0 - ETA: 3s - loss: 0.1840 - f1: 0.8160 - precision: 0.7627 - recall: 0.8763 - auc: 0.8035 - prc: 0.745 - ETA: 3s - loss: 0.1840 - f1: 0.8160 - precision: 0.7627 - recall: 0.8763 - auc: 0.8035 - prc - ETA: 3s - loss: 0.1840 - f1: 0.8160 - precision: 0.7627 - recall: 0.8763 - auc: 0.8035 - ETA: 2s - loss: 0.1841 - f1: 0.8159 - precision: 0.7627 - recall: 0.8763 - auc: 0.8035 - - ETA: 2s - loss: 0.1841 - f1: 0.8159 - precision: 0.7627 - recall: 0.8763 - auc: 0.8035 - prc: 0.745 - ETA: 2s - loss: 0.1841 - f1: 0.8159 - precision: 0.7627 - reca\n — val_f1: 0.39655966825372446 — val_precision: 0.2657550360241141, — val_recall: 0.7809367438645005\nEpoch 122/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1835 - f1: 0.8165 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - prc: 0.7451A: 7s - loss: 0.1779 - f1: 0.8221 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - prc: 0. - ETA: 7s - loss: 0.1786 - f1: 0.8214 - precision: 0.7627 - recall: 0.8764 - au - ETA: 6s - loss: 0.1824 - f1: 0.8176 - precision: 0.7627 - recall: 0.8764 - auc:  - ETA: 5s - loss: 0.1833 - f1: 0.8166 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - ETA: 4s - loss: 0.1835 - f1: 0.8165 - precision: 0.7627 - recall: 0.8764 - auc - ETA: 3s - loss: 0.1834 - f1: 0.8166 - precision: 0.7627 - recall: 0.8764 - auc: 0.803 - ETA: 2s - loss: 0.1834 - f1: 0.8166 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - prc: 0.74 - ETA: 2s - loss: 0.1834 - f1: 0.8166 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - - ETA: 2s - loss: 0.1833 - f1: 0.8166 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - prc: 0. - ETA: 1s - loss: 0.1834 - f1: 0.8166 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - prc: 0 - ETA: 1s - loss: 0.1834 - f1: 0.8166 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - ETA: 0s - loss: 0.1834 - f1: 0.8166 - precision: 0.7627 - recall: 0.8764 - auc: 0.8\n — val_f1: 0.39807558342041105 — val_precision: 0.26606332208124783, — val_recall: 0.7900967853439337\nEpoch 123/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1832 - f1: 0.8168 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - prc: 0.7452A: 7s - loss: 0.1850 - f1: 0.8149 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - - ETA: 7s - loss: 0.1844 - f1: 0.8156 - precision: 0.7627 - recall: 0.8764 -  - ETA: 5s - loss: 0.1838 - f1: 0.8162 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 -  - ETA: 5s - loss: 0.1836 - f1: 0.8163 - precision: 0.7627 - reca - ETA: 3s - loss: 0.1833 - f1: 0.8167 - precision: 0.7627 - recall: 0.8764 - auc: 0.80 - ETA: 2s - loss: 0.1832 - f1: 0.8167 - precision: 0.7627 - recall: 0.8764 - auc: 0.8 - ETA: 1s - loss: 0.1832 - f1: 0.8168 - precision: 0.7627 - recall: 0.8764 -  - ETA: 0s - loss: 0.1832 - f1: 0.8168 - precision: 0.7627 - recall: 0.8764 - auc: 0.8035 - prc\n — val_f1: 0.3852130325814536 — val_precision: 0.2539936102236422, — val_recall: 0.7969236087106809\nEpoch 124/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1827 - f1: 0.8173 - precision: 0.7628 - recall: 0.8763 - auc: 0.8035 - prc: 0.7453A: 6s - loss: 0.1839 - f1: 0.8161 - precision: 0.7628 - recall: 0.8763 - au - ETA: 5s - loss: 0.1832 - f1: 0.8168 - precision: 0.7628 - recall: 0.8763 - auc: 0.8035  - ETA: 4s \n — val_f1: 0.40048645867440014 — val_precision: 0.27454627076252164, — val_recall: 0.7398893881783616\nEpoch 125/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1902 - f1: 0.8098 - precision: 0.7629 - recall: 0.8762 - auc: 0.8035 - prc: 0.7453A: 7s - loss: 0.1927 - f1: 0.8073 - precision: 0.7629 - recall: 0.8762 - auc: 0.8036 - prc - ETA: 7s - loss: 0.1925 - f1: 0.8075 - precision: 0.7629 - recall: 0.8762 - auc: 0.8036 - prc: 0.745 - ETA: 7s - loss: 0.1925 - f1: 0.8075 - precision: 0.7629 - recall: 0.8762 - auc: 0.8036 - prc: 0.74 - ETA: 7s - loss: 0.1924 - f1: 0.8076 - precision: 0.7629 - recall: 0.8762 - auc: 0.8036 - prc: 0. - ETA: 7s - loss: 0.1924 - f1: 0.8077 -  - ETA: 3s - loss: 0.1917 - f1: - ETA: 0s - loss: 0.1903 - f1: 0.8097 - precision: 0.7629 - recall: 0.8762 - auc: 0.8035 - prc:\n — val_f1: 0.3974171958125965 — val_precision: 0.2643248487615569, — val_recall: 0.8004666436225372\nEpoch 126/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1808 - f1: 0.8192 - precision: 0.7629 - recall: 0.8763 - auc: 0.8035 - prc: 0.7453A: 8s - loss: 0.1749 - f1: 0.8251 - precision: 0.7628 - recall: 0.8763 - auc: 0.8 - ETA: 7s - loss: 0.1766 - f1: 0.8234 - precision: 0.7628 - recall: 0.8763 - auc: 0.8035 - prc: - ETA: 7s - loss: 0.1772 - f1: 0.8228 - precision:  - ETA: 4s - loss: 0.1793 - f1: 0.8207 - precision: 0.7629 - recall: 0.8763 - auc: 0.8035 - prc:  - ETA: 4s - loss: 0.1794 - f1: 0.8206 - precision: 0.7629 - recall: 0.8763 - auc: 0.8035 - prc: 0.7 - ETA: 4s - loss: 0.1795 - f1: 0.8205 - precision: 0.7629 - recal - ETA: 2s - loss: 0.1800 - f1: 0.8200 - precision: 0.7629 - recall: 0.8763  - ETA: 0s - loss: 0.1805 - f1: 0.8195 - precision: 0.7629 - recall: 0.8763 - auc: 0.\n — val_f1: 0.3759645250141626 — val_precision: 0.24288851308715514, — val_recall: 0.8315762184583477\nEpoch 127/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1886 - f1: 0.8114 - precision: 0.7629 - recall: 0.8762 - auc: 0.8035 - prc: 0.7453A: 7s - loss: 0.1839 - f1: 0.8161 - precision: 0.7629 - recall: 0.8763 - auc: 0. - ETA: 6s - loss: 0.1845 - f1: 0.8155 - pre - ETA: 3s - loss: 0.1875 - f1: 0.8125 - precision: 0.7629 - recall: 0.8763 - auc: 0.8035 -  - ETA: 2s - loss: 0.1879 - f1: 0.8121 - precision: \n — val_f1: 0.38434712084347117 — val_precision: 0.2510861502596164, — val_recall: 0.8190459730383685\nEpoch 128/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1832 - f1: 0.8168 - precision: 0.7630 - recall: 0.8761 - auc: 0.8035 - prc: 0.7453A: 8s - loss: 0.1796 - f1: 0.8204 - precision: 0.7630 - recall: 0.8761 - auc: 0 - ETA: 2s - loss: 0.1834 - f1: 0.8166 - precision: 0.7630 - recall: 0.8761 - auc: - ETA: 1s - loss: 0.1833 - f1: 0.8167 - precision: 0.7630 - recall: 0.8761 - auc: 0.8035 - - ETA: 0s - loss: 0.1833 - f1: 0.8167 - precision: 0.7630 - recall: 0.8761 - auc: 0.8035 - prc: 0.74 - ETA: 0s - loss: 0.1833 - f1: 0.8167 - precision: 0.7630 - recall: 0.8761 - auc: 0.8035 - p\n — val_f1: 0.37141352987628323 — val_precision: 0.2373480078819628, — val_recall: 0.8535257518147252\nEpoch 129/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1817 - f1: 0.8183 - precision: 0.7630 - recall: 0.8762 - auc: 0.8035 - prc: 0.7453A: 8s - loss: 0.1832 - f1: 0.8168 - precision: 0.7630 - recall: 0.8761 - auc: 0.8035 - prc: - ETA: 7s - loss: 0.1818 - f1: 0.8182 - precision: 0.7630 - rec - ETA: 5s - loss: 0.1796 - f1: 0.8204 - precision: 0.7630 - recall: 0.8762 - auc: 0.8035 - prc: - ETA: 5s - loss: 0.1797 - f1: 0.8203 - precision: 0.7630 - recall: 0.8762 - auc: 0.8035 - prc: 0.745 - ETA: 5s - loss: 0.1797 - f1: 0.8203 - precision: 0.7630 - recall: 0.8762 - auc: 0.8035 - prc:  - ETA: 5s - loss: 0.1798 - f1: 0.8202 - precision: 0.7630 - recall: 0.8762 - auc: 0.8035 - prc: 0.745 - ETA: 5s - loss: 0.1798 - f1: 0.8202 - pre - ETA: 1s - loss: 0.1808 - f1: 0.8192 - precision: 0.7630 - recall: 0.8762 - auc: 0.80 - ETA: 1s - loss: 0.1812 - f1: 0.8188 - precision: 0.7630 - recall: 0.8762 - auc: 0.803 - ETA: 0s - loss: 0.1816 - f1: 0.8184 - precision: 0.7630 - recall: 0.8762 - auc: 0.8035 - prc\n — val_f1: 0.40612349355544186 — val_precision: 0.27787328876154094, — val_recall: 0.7542343587970964\nEpoch 130/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 9s 1ms/step - loss: 0.1877 - f1: 0.8122 - precision: 0.7630 - recall: 0.8761 - auc: 0.8035 - prc: 0.7453A: 6s - loss: 0.1916 - f1: 0.8084 - precision: 0.7630 - recall: 0.8761 - auc: 0.8035 - prc: 0.7 - ETA: 6s - loss: 0.1915 - f1: 0.8085 - precision: 0.7630 - recall: 0.8761 - auc: 0.8035 - ETA: 5s - loss: 0.1908 - f1: 0.8092 - precision: 0.7630 - recall: 0.8761 - auc: 0.8035 - prc - ETA: 5s - lo - ETA: 0s - loss: 0.1879 - f1: 0.8120 - precision: 0.7630 - recall: 0.8761 - auc: 0.8035 - \n — val_f1: 0.3556536322711625 — val_precision: 0.2234819071284361, — val_recall: 0.870463187003111\nEpoch 131/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1845 - f1: 0.8155 - precision: 0.7630 - recall: 0.8762 - auc: 0.8035 - prc: 0.7453A: 6s - loss: 0.1894 - f1: 0.8106 - precision: 0.7630 - rec - ETA: 4s - loss: 0.1869 - f1: 0.8131 - precision: 0.7630 - recall: 0.8762 - auc: 0.8035 - p - ETA: 4s - loss: 0.1864 - f1: 0.8136 - precision: 0.7630 - recall: 0.8762 - auc: 0.803 - ETA: 3s - loss: 0.1859 - f\n — val_f1: 0.3587411707615162 — val_precision: 0.22647332471010054, — val_recall: 0.8624265468371932\nEpoch 132/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1813 - f1: 0.8187 - precision: 0.7630 - recall: 0.8763 - auc: 0.8035 - prc: 0.7453A: 7s - loss: 0.1823 - f1: 0.8177 - precision: 0.7630 - recall: 0.8762 - auc: 0.8035 - prc: - ETA: 7s - loss: 0.1819 - f1: 0.8181 - precision: 0.7630 - recall: 0.8762 - auc: 0.8035 - prc:  - ETA: 6s - loss: 0.1816 - f1: 0.8184 - precision: 0.7630 - recall: 0.8762 - auc: 0. - ETA: 5s - loss: 0.1811 - f1: 0.8189 - precision: 0.7630 - recall: 0.8763 - auc: 0.8035 - prc: 0.74 - ETA: 5s - loss: 0.1811 - f1: 0.8189 - precision: 0.7630 - recall: 0.8763 - auc: 0.8035 - prc: 0.7 - ETA: 5s - loss: 0.1810 - f1: 0.8190 - precision: 0.7630 - recall: 0.8763 - - ETA: 4s - loss: 0.1810 - f1: 0.8190 - precision: 0.7630 - recall: 0.8763 - auc: 0.8035 - prc:  - ETA: 4s - loss: 0.1810 - f1: 0.8190 - precision: 0.7630 - rec - ETA: 2s - loss: 0.1813 - f1: 0.8187 - precision: 0.7630 - recall: 0.8763 - auc: 0.8 - ETA: 1s - loss: 0.1813 - f1: 0.8187 - precision: 0.7630 - recall: 0.8763 - auc:  - ETA: 0s - loss: 0.1813 - f1: 0.8187 - precision: 0.7630 - recall: 0.8763 - auc: 0.8035 - prc: 0.74 - ETA: 0s - loss: 0.1813 - f1: 0.8187 - precision: 0.7630 - recall: 0.8763 - auc: 0.8035 - prc: 0.7\n — val_f1: 0.37641563131759215 — val_precision: 0.24351778957259315, — val_recall: 0.8286380919460767\nEpoch 133/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1808 - f1: 0.8192 - precision: 0.7630 - recall: 0.8763 - auc: 0.8036 - prc: 0.7453A: 8s - loss: 0.1746 - f1: 0.8254 - precision: 0.7630 - recall: 0.8763 - auc: 0.8036 - prc: 0 - ETA: 8s - loss: 0.1774 - f1: 0.8227 - precision: 0.7630 - recall: 0.8763 - auc: 0.80 - ETA: 7s - loss: 0.1791 - f1: 0.8209 - precision: 0.7630 - - ETA: 5s - loss: 0.1808 - f1: 0.8192 - precision: 0.7630 - recall: 0.8763 - auc: 0.8036 - p - ETA: 4s - loss: 0.1808 - f1: 0.8192 - precision: 0.7630 - recall: 0.8763 - auc: 0.8036 - prc: 0. - ETA: 4s - loss: 0.1808 - f1: 0.8192 - precision: 0.7630 - recall: 0.8763 - auc: 0.8036 - prc: - ETA: 4s - loss: 0.1808 - f1: 0.8192 - precision: 0.7630 - recall: 0.8763 - auc: 0.8036 - ETA: 3s - loss: 0.1808 - f1: 0.8192 - precision: 0.7630 - r - ETA: 1s - loss: 0.1808 - f1: 0.8192 - precision: 0.7630 - recall: 0.8763 - auc: 0.8036 - prc: 0.745 - ETA: 1s - loss: 0.1808 - f1: 0.8192 - precision: 0.7630 - recall: 0.8763 - a\n — val_f1: 0.3924126455906822 — val_precision: 0.26397636032594096, — val_recall: 0.7642585551330798\nEpoch 134/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1855 - f1: 0.8145 - precision: 0.7631 - recall: 0.8763 - auc: 0.8036 - prc: 0.7454A: 6s - loss: 0.1910 - f1: 0.8090 - precision: 0.7631 - recall: 0.8763 - auc: 0.8036 - prc: 0 - ETA: 6s - loss: 0.1905 - f1: 0.8095 - precision: 0.7631 - recall: 0.8763 - auc: 0.8 - ETA: 5s - loss: 0.1890 - f1: 0.8110 - precision: 0.7631 - recall: 0.8763 - auc: 0.8036 - prc: 0.745 - ETA: 5s - loss: 0.1889 - f1: 0.8111 - precision: 0.7631 - recall: 0.8763 - auc: 0.8036 - prc: 0.745 - ETA: 5s - loss: 0.1888 - f1: 0.8112 - precision: 0.7631 - recall: 0.8763 - auc: 0.8036 - prc:  - ETA: 5s - loss: 0.1884 - f1: 0.8116 - precision: 0.7631 - recall: 0.8763 - a - ETA: 3s - loss: 0.1873 - f1: 0.8127 - precision: 0.7631 - recall: 0.8 - ETA: 2s - loss: 0.1863 - f1: 0.8137 - precision: 0.7631 - recall: 0.8763 - auc: - ETA: 1s - loss: 0.1859 - f1: 0.8141 - precision: 0.7631 - recall: 0.8763 - \n — val_f1: 0.3694167906320327 — val_precision: 0.2370172776662914, — val_recall: 0.8369339785689596\nEpoch 135/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1844 - f1: 0.8157 - precision: 0.7630 - recall: 0.8763 - auc: 0.8036 - prc: 0.7454A: 7s - loss: 0.1839 - f1: 0.8162 - precision: 0.7631 - recall: 0.8763 - auc: 0.8036 - prc:  - ETA: 7s - loss: 0.1834 - f1: 0.8166 - precision: 0.7631 - recall: 0.8763 - auc: 0.8036 -  - ETA: 6s - loss: 0.1832 - f1: 0.8169 - precision: 0.7631 - recall: 0.8763 - auc: 0.8036 - prc: - ETA: 6s - loss: 0.1832 - f1: 0.8168 - precision: 0.7631  - ETA: 4s - loss: 0.1838 - f1: 0.8162 - precision: 0.7631 - recall: 0.8763 - auc: 0.8036 - p - ETA: 3s - loss: 0.1841 - f1: 0.8159 - precision: 0.7631 - recall: 0.8763 - auc: 0.8036 - p - ETA: 3s - loss: 0.1843 - f1: 0.8157 - precision: 0.7631 - recall: 0.8763 - auc: 0.803 - ETA: 2s - loss: 0.1845 - f1: 0.8155 - precision: 0.76\n — val_f1: 0.3843511296124156 — val_precision: 0.25083082766260484, — val_recall: 0.8218112685793294\nEpoch 136/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1782 - f1: 0.8218 - precision: 0.7630 - recall: 0.8765 - auc: 0.8036 - prc: 0.7453A: 8s - loss: 0.1789 - f1: 0.8211 - precision: 0.7630 - recall: 0.8764 - auc: 0.8036 - prc:  - ETA: 8s - loss: 0.1784 - f1: 0.8216 - precision: 0.7630 - recall: 0.876 - ETA: 6s - loss: 0.1775 - f1: 0.8225 - precision: 0.7630 - recall - ETA\n — val_f1: 0.36536184697350893 — val_precision: 0.23244044816872234, — val_recall: 0.8533529208434152\nEpoch 137/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1821 - f1: 0.8179 - precision: 0.7631 - recall: 0.8765 - auc: 0.8037 - prc: 0.7454\n — val_f1: 0.37668057037540015 — val_precision: 0.24286393315487953, — val_recall: 0.8389215347390252\nEpoch 138/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1812 - f1: 0.8188 - precision: 0.7631 - recall: 0.8765 - auc: 0.8037 - prc: 0.7454A: 5s - loss: 0.1805 - f1: 0.8195 - precision: 0.7631 - recall: 0.8764 - auc - ETA: 4s - loss: 0.1808 - f1: 0.8192 - precision: 0.7631 - recall: 0.8765 - auc: 0.8037 - pr - ETA: 4s - loss: 0.1809 - f1: 0.8191 - precision: 0.7631 - recall: - ETA: 2s - loss: 0.1812 - f1: 0.8189 - precision: 0.7631 - recall: 0.8765 - auc: 0.8037 - prc: 0. - ETA: 2s - loss: 0.1812 - f1: 0.8189 - precision: 0.7631 - recall: 0.8765 - auc: 0.80 - ETA: 1s - loss: 0.1812 - f1: 0.8188 - precision: 0.7631 - recall: 0.87\n — val_f1: 0.39020147847514136 — val_precision: 0.2565834263772773, — val_recall: 0.8142067058416869\nEpoch 139/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1831 - f1: 0.8170 - precision: 0.7631 - recall: 0.8765 - auc: 0.8037 - prc: 0.7454A: 8s - loss: 0.1845 - f1: 0.8155 - precision: 0.7631 - recall: 0.8765 - auc: 0.8037 - prc:  - ETA: 8s - loss: 0.1833 - f1: 0.8167 - precision: 0 - ETA: 5s - loss: 0.1826 - f1: 0.8174 - precision: 0.7631 - recall: 0.8765 - auc: 0.8037 - - ETA: 4s - loss: 0.1827 - f1: 0.8174 - precision: 0.7631 - recall: 0.8765 - auc: 0.8037 - prc: 0.74 - ETA: 4s - loss: 0.1827 - f1: 0.8174 - precision: 0.7631 - recall: 0.8765 - auc: 0.8037 - prc:  - ETA: 4s - loss: 0.1827 - f1: 0.8174 - precision: 0.7631 - recall: 0.87 - ETA: 2s - loss: 0.1827 - f1: 0.8173 - precis\n — val_f1: 0.35597671777399204 — val_precision: 0.22398392139347922, — val_recall: 0.8667473211199447\nEpoch 140/150\n"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "6213/6213 [==============================] - 9s 2ms/step - loss: 0.1847 - f1: 0.8153 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - prc: 0.7454A: 9s - loss: 0.1859 - f1:\n — val_f1: 0.3738092505451624 — val_precision: 0.24003832358866015, — val_recall: 0.8443657103352921\nEpoch 141/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1850 - f1: 0.8150 - precision: 0.7630 - recall: 0.8767 - auc: 0.8037 - prc: 0.7454: 4s - loss: 0.1842 - f1: 0.8158 - precision: 0.7630 - recall: 0.8767 -  - ETA: 2s - loss: 0.1846 - f1: 0.8154 - precision: 0.7630 - reca\n — val_f1: 0.38556027044037927 — val_precision: 0.25198375839282394, — val_recall: 0.820515036294504\nEpoch 142/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1842 - f1: 0.8158 - precision: 0.7630 - recall: 0.8766 - auc: 0.8037 - prc: 0.7454A: 8s - loss: 0.1828 - f1: 0.8170 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - prc: 0.74 - ETA: 8s - loss: 0.1838 - f1: 0.8161 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - prc: 0.745 - ETA: 8s - loss: 0.1841 - f1: 0.8158 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - prc: 0.7 - ETA: 8s - loss: 0.1842 - f1: 0.8157 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - - ETA: 7s - loss: 0.1846 - f1: 0.8153 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - prc:  - ETA: 7s - loss: 0.1848 - f1: 0.8152 - p - ETA: 4s - loss: 0.1846 - f1: 0.8154 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - prc: 0.745 - ETA: 4s - loss: 0.1846 - f1: 0.8154 - precision: 0.7631 - recall: 0.8766 - a - ETA: 2s - loss: 0.1845 - f1: 0.8155 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - prc: 0. - ETA: 2s - loss: 0.1845 - f1: 0.8155 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - prc: 0.745 - ETA: 2s - loss: 0.1845 - f1: 0.8155 - precision: 0\n — val_f1: 0.3910533311181259 — val_precision: 0.2573810825587753, — val_recall: 0.8136017974421016\nEpoch 143/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1798 - f1: 0.8202 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - prc: 0.7454A: 7s - loss: 0.1765 - f1: 0.8234 - precision: 0.7631 - recall: 0.8766 - auc: 0.8037 - prc: 0.745 - ETA: 7s - loss: 0.1766 - f1: 0.8233 - precision: 0\n — val_f1: 0.3860702128945195 — val_precision: 0.25170050161514823, — val_recall: 0.8282060145178016\nEpoch 144/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1804 - f1: 0.8196 - precision: 0.7631 - recall: 0.8768 - auc: 0.8037 - prc: 0.7454A: 8s - loss: 0.1778 - f1: 0.8222 - precision: 0.7630 - recall: 0.8768 - auc:  - ETA: 7s - loss: 0.1790 - f1: 0.8 - ETA: 3s - loss: 0.1800 - f1: 0.8200 - precision: 0.7631 - recall:  - ETA: 1s - loss: 0.1803 - f1: 0.8197 - precision: 0.7631 - recall: 0.8768 - auc: 0.8037 - ETA: 1s - loss: 0.1804 - f1: 0.8196 - precision: 0.7631 - recall: 0.8768 - auc: 0.8037 - prc: 0.745 - ETA: 0s - loss: 0.1804 - f1: 0.8196 - precision: 0.7631 - recall: 0.8768 - auc:\n — val_f1: 0.39966804272885903 — val_precision: 0.2651177234487042, — val_recall: 0.8115278257863809\nEpoch 145/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1785 - f1: 0.8215 - precision: 0.7632 - recall: 0.8767 - auc: 0.8037 - prc: 0.7454A: 8s - loss: 0.1756 - f1: 0.8244 - precision: 0.7631 - recall: 0.876 - ETA: 7s - loss: 0.1770 - f1: 0.8230 - precision: 0.7631 - recall: 0.8767 - auc: 0.8037 - prc: 0.745 - ETA: 6s - loss: 0.1771 - f1: 0.8229 - precision: 0.7631 - recall: 0.8767 - auc: 0.8037 - prc: 0 - ETA: 6s - loss: 0.1773 - f1: 0.8227 - precision: 0.7631 - recall: 0.8767 - auc: 0. -\n — val_f1: 0.3887603170377046 — val_precision: 0.25475775063749834, — val_recall: 0.8201693743518839\nEpoch 146/150\n6213/6213 [==============================] - 10s 2ms/step - loss: 0.1840 - f1: 0.8160 - precision: 0.7632 - recall: 0.8767 - auc: 0.8038 - prc: 0.7455: 8s - loss: 0.1756 - f1: 0.8243 - precision: 0.7632 - recall: 0.8767 - au - ETA: 7s - loss: 0.1813 - f1: 0.8186 - precision: 0.7632 - recall: 0.8767 - auc: 0.8038 - - ETA: 6s - loss: 0.1822 - f1: 0.8178 - precision: 0.7632 - recall: 0.8767 - auc: 0. - ETA: 5s - loss: 0.1827 - f1: 0.8173 - precision: 0.7632 - recall: 0.8767 - auc: 0.8038 - pr - ETA: 5s - loss: 0.1828 - f1: 0.8172 - precision: 0.7632 - recall: 0.8767 - auc: 0.8038 - prc: 0.7 - ETA: 5s - loss: 0.1828 - f1: 0.8172 - precision: 0.7632 - recall: 0.8767 - - ETA: 3s - loss: 0.1832 - f1: 0.8168 - precision: 0.7632 - recall: 0.8767 - auc: 0.8038 - prc: - ETA: 3s - loss: 0.1832 - f1: 0.8167 - precision: 0.7632 - recall: 0.8767 - auc: 0.8038 - prc: 0. - ETA: 3s - loss: 0.1833 - f1: 0.8167 - precision: 0.7632 - recall: 0.8767 -  - ETA: 2s - loss: 0.1837 - f1: 0.8163 - precision:\n — val_f1: 0.3915877761169241 — val_precision: 0.25770576019237074, — val_recall: 0.8149844452125821\nEpoch 147/150\n6213/6213 [==============================] - 9s 2ms/step - loss: 0.1827 - f1: 0.8173 - precision: 0.7633 - recall: 0.8766 - auc: 0.8038 - prc: 0.7456A: 3s - loss: 0.1809 - f1: 0\n — val_f1: 0.3909660359364501 — val_precision: 0.2609286850860697, — val_recall: 0.77938126512271\nEpoch 148/150\n6213/6213 [==============================] - 9s 2ms/step - loss: 0.1799 - f1: 0.8201 - precision: 0.7634 - recall: 0.8765 - auc: 0.8039 - prc: 0.7456A: 1s - loss: 0.1796 - f1: 0.8204 - precision: 0.7634 - recall: \n — val_f1: 0.40054965428490413 — val_precision: 0.26789204959883295, — val_recall: 0.7934669892844798\nEpoch 149/150\n6213/6213 [==============================] - 8s 1ms/step - loss: 0.1809 - f1: 0.8191 - precision: 0.7635 - recall: 0.8764 - auc: 0.8039 - prc: 0.7457\n — val_f1: 0.3958720955389209 — val_precision: 0.264040404040404, — val_recall: 0.7906152782578638\nEpoch 150/150\n6213/6213 [==============================] - 9s 1ms/step - loss: 0.1829 - f1: 0.8171 - precision: 0.7636 - recall: 0.8763 - auc: 0.8039 - prc: 0.7458A: 6s - loss: 0.1836 - f1: 0.8164 - precision: 0.7636 - recall: 0.8763 - auc: 0.8039 - prc: 0. - ETA: 6s - loss: 0.1836 - f1: 0.8164 - precision: 0.7636 - recall: 0.8763 - auc: 0.80 - ETA: 5s - loss: 0.1836 - f1: 0.8164 - precision:  - ETA: 2s - loss: 0.1834 - f1: 0.8166 - precision: 0.7636 - recal - ETA: 0s - loss: 0.1831 - f1: 0.8169 - precision: 0.7636 - recall: 0.8763 - auc: 0.8\n — val_f1: 0.400957589334739 — val_precision: 0.26879472304838187, — val_recall: 0.7888005530591082\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Results",
   "metadata": {
    "cell_id": "00037-39a72b4e-f99d-4375-83ce-3ee2d19afa94",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00038-dd51a2df-5396-4352-8462-bbdf8f6865d9",
    "deepnote_cell_type": "code"
   },
   "source": "test_results = model.evaluate(testX, testy, verbose=0)\nfor x, y in zip(model.metrics_names, test_results):\n  print(x, ': ', y)\nprint()",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "loss :  0.6198608875274658\nf1 :  0.3801007568836212\nprecision :  0.7631115317344666\nrecall :  0.8763002157211304\nauc :  0.803955078125\nprc :  0.7452502250671387\n\n"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Feature importance",
   "metadata": {
    "cell_id": "00039-24f33414-5a2e-44cc-afd4-7c00ffcd0b62",
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00040-b40e6c7c-e9aa-4844-a501-87d5dfef36d9",
    "deepnote_cell_type": "code"
   },
   "source": "trainX_raw, validateX_raw, testX_raw, trainy_raw, validatey_raw, testy_raw = split_dataset(df,sample=1)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00041-a96abaef-3d05-4f0e-9956-38ea30cd03b1",
    "deepnote_cell_type": "code"
   },
   "source": "my_model = KerasRegressor(build_fn=prepare_model)    \nmy_model.fit(trainX,trainy)\n\nperm = PermutationImportance(my_model, random_state=1).fit(trainX,trainy)\neli5.show_weights(perm, feature_names = trainX_raw.columns.tolist())",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00042-e0ba6caf-fea1-4b79-83ad-b676b33d4fe8",
    "deepnote_cell_type": "code"
   },
   "source": "eli5.show_weights(perm, feature_names = trainX_raw.columns.tolist())",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/html": "\n    <style>\n    table.eli5-weights tr:hover {\n        filter: brightness(85%);\n    }\n</style>\n\n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n        <table class=\"eli5-weights eli5-feature-importances\" style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto;\">\n    <thead>\n    <tr style=\"border: none;\">\n        <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">Weight</th>\n        <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n    </tr>\n    </thead>\n    <tbody>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.1126\n                \n                    &plusmn; 0.0012\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                combined_followers\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 91.84%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0313\n                \n                    &plusmn; 0.0003\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                loudness\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 92.88%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0258\n                \n                    &plusmn; 0.0007\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                danceability\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 95.14%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0149\n                \n                    &plusmn; 0.0004\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                valence\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 95.36%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0140\n                \n                    &plusmn; 0.0004\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                acousticness\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 96.50%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0093\n                \n                    &plusmn; 0.0006\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                duration_ms\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 96.79%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0082\n                \n                    &plusmn; 0.0004\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                energy\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 97.33%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0063\n                \n                    &plusmn; 0.0003\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                release_day\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 97.93%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0044\n                \n                    &plusmn; 0.0003\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                explicit\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 98.62%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0025\n                \n                    &plusmn; 0.0002\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                speechiness\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 98.69%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0023\n                \n                    &plusmn; 0.0003\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                liveness\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 98.78%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0021\n                \n                    &plusmn; 0.0002\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                instrumentalness\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 98.82%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0020\n                \n                    &plusmn; 0.0004\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                release_month\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 98.85%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0019\n                \n                    &plusmn; 0.0002\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                tempo\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 98.97%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0016\n                \n                    &plusmn; 0.0003\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                #artists\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.33%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0009\n                \n                    &plusmn; 0.0003\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                time_signature\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.38%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0008\n                \n                    &plusmn; 0.0003\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                mode\n            </td>\n        </tr>\n    \n        <tr style=\"background-color: hsl(120, 100.00%, 99.66%); border: none;\">\n            <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n                0.0003\n                \n                    &plusmn; 0.0003\n                \n            </td>\n            <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n                key\n            </td>\n        </tr>\n    \n    \n    </tbody>\n</table>\n    \n\n    \n\n\n    \n\n    \n\n    \n\n    \n\n    \n\n    \n\n\n\n",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cell_id": "00043-ee82b748-8987-498d-87b7-a6218c825eb3",
    "deepnote_cell_type": "code"
   },
   "source": "",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=efdb1931-d19c-4850-b12b-726a7087f8c0' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "fb1f7e59-a2cf-4e67-a4eb-d0a2cd5b3e18",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 }
}